# Robotics Vision & Calibration Suite

A modular, production-grade Python toolkit for robotics calibration, vision, 3D point cloud processing, and automated pose/trajectory collection.

This repository groups several focused modules under a single project umbrella. Each individual components remain testable and extensible. The sections below describe the directory layout and how the pieces fit together.

**Supports:**

* Robot high-level API (move, home,  state, RPC)
* RealSense 3D camera management
* Automated Charuco and hand-eye calibration (OpenCV)
* 3D point cloud generation, transformation, and visualization
* CLI tools for pose recording, trajectory execution, calibration, camera debug, and cloud ops
* Flexible, extensible logger with JSON/file support

---

## ðŸ“‚ Project Structure (Navigator)

```
project-root/
â”‚
â”œâ”€â”€ calibration/          # Calibration algorithms & workflows
â”‚   â”œâ”€â”€ charuco.py              # Charuco board calibration helpers
â”‚   â”œâ”€â”€ handeye.py              # Hand-eye calibration helpers
â”‚   â”œâ”€â”€ pose_loader.py          # Load robot poses from JSON
â”‚   â”œâ”€â”€ workflows.py            # High-level calibration routines
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ robot/                # Robot API & workflows
â”‚   â”œâ”€â”€ controller.py           # Main control class
â”‚   â”œâ”€â”€ workflows.py            # Pose recorder and path runner
â”‚   â”œâ”€â”€ marker.py               # Simple marker utilities
â”‚   â”œâ”€â”€ Robot.py                # Cython RPC bindings
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ utils/                # Common utilities
â”‚   â”œâ”€â”€ config.py               # Config loading/abstraction
â”‚   â”œâ”€â”€ logger.py               # Centralized, JSON-capable logger
â”‚   â”œâ”€â”€ io.py                   # Camera calibration I/O
â”‚   â”œâ”€â”€ geometry.py             # Math helpers
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ vision/               # Vision, cloud, and camera utils
â”‚   â”œâ”€â”€ opencv_utils.py         # OpenCV helper class
â”‚   â”œâ”€â”€ realsense.py            # RealSense camera wrapper
â”‚   â”œâ”€â”€ cloud/                  # Point cloud subpackage
â”‚   â”‚   â”œâ”€â”€ generator.py            # PointCloudGenerator class
â”‚   â”‚   â”œâ”€â”€ aggregator.py           # Multi-frame cloud builder
â”‚   â”‚   â””â”€â”€ pipeline.py             # Filtering/analysis helpers
â”‚   â”œâ”€â”€ tools.py                # Camera and cloud helper routines
â”‚   â”œâ”€â”€ transform.py            # 3D transformation utilities
â”‚   â””â”€â”€ README.md               # Explanation of transform chain
â”‚
â”œâ”€â”€ config.yaml           # Main configuration file
â”œâ”€â”€ pyproject.toml        # Project metadata & dependencies
â””â”€â”€ README.md             # You are here
```

---

## Overview

The project is organized into four main packages that mirror typical robotics layers:

* **calibration/** â€“ Charuco board detection and handâ€‘eye calibration. The mathematical background of the pose estimation and the classical `AX = XB` formulation are explained in [calibration/README.md](calibration/README.md).
* **robot/** â€“ Robot connection logic and highâ€‘level motion API. Communication is decoupled from workflows so hardware can be replaced without touching the algorithm code (Dependency Inversion Principle).
* **vision/** â€“ Camera interfaces, point cloud tools, and coordinate transforms. Transform chains and coordinate conventions appear in [vision/README.md](vision/README.md).
* **utils/** â€“ Shared helpers for configuration, logging and geometry calculations.

Every component keeps a single responsibility and exposes a minimal interface. New robots or cameras can be integrated by implementing the same interfaces without modifying existing modules.

---

## ðŸš€ Quickstart

### 1. Build & Install

Use `pyproject.toml` together with [uv](https://github.com/astral-sh/uv) for reproducible environments/builds:

```bash
uv venv .venv -p 3.12
uv pip install -e .
```

All dependencies are defined in `pyproject.toml`.

### 2. Configure

Edit `config.yaml` for your robot/camera IP, tool, velocity, logging, and point cloud settings.

### 3. Run CLI Tools
CLI modules are thin wrappers calling workflow helpers under
`calibration/`, `robot/`, and `vision/`.

* Save a pose:

  ```bash
  poses-saver
  ```
* Run a trajectory from file:

  ```bash
  path-runner
  ```
* Calibrate camera (Charuco):

  ```bash
  charuco-calib
  ```
* Capture point cloud:

  ```bash
  pointcloud-capture --output clouds/cloud.ply
  ```

---

## ðŸ“‘ Documentation

### Configuration (`config.yaml`)

* `robot:` â€” IP, tool/user frame, velocity, emergency delay
* `vision:` â€” RealSense stream parameters, cloud parameters
* `logging:` â€” log directory, level, JSON output
* `cloud:` â€” point cloud setting, (resolution, voxel size, output dir, ...)
* Defaults for paths, robot IP, and Charuco dictionary mapping are defined in `utils/constants.py`

### Logger (`utils/logger.py`)

* Central logger: file/console, JSON/text, auto-timestamped logs
* Decorators for function/exception logging
* Use in all modules:

  ```python
  from utils.logger import Logger
  logger = Logger.get_logger("my.module", json_format=True)
  ```

### Robot API (`robot/controller.py`)

* `RobotController` â€” High-level API: connect, move, record, return home, shutdown
* Uses config + logger

### Calibration Modules

* `CharucoCalibrator` â€” Board-based camera calibration (OpenCV ArUco)
* `HandEyeCalibrator` â€” Tsai/Daniilidis methods, modular savers for results

### Vision

* `RealSenseCamera` â€” Start/stop, get frames, intrinsics, depth scale
* `opencv_utils.py` â€” draw\_text, normalize\_depth, apply\_colormap

### 3D Point Cloud Modules

* `cloud/generator.py` â€” Depth to point cloud conversion, save/load (PLY, XYZ, npz)
* `cloud/aggregator.py` â€” Multi-frame cloud assembly with optional ICP
* `cloud/pipeline.py` â€” Filtering, clustering, and trajectory analysis
* `transform.py` â€” Rigid transformations between camera/robot/world coordinates (uses calibration, see vision/README.md)
* CLI scripts provided via entry points (`pointcloud-capture`, `pointcloud-transform`, `pointcloud-view`)

**See [vision/README.md](./vision/README.md) for detailed usage, coordinate system details, and all math for 3D transforms.**

**Typical workflow:**

1. Capture depth + color â†’ generate cloud (`pointcloud-capture`)
2. (Optional) Filter, merge, or transform clouds (robot <-> camera <-> world)
3. Save or visualize result (`pointcloud-view`)

## Mathematical Background

Camera calibration uses OpenCV's implementation of Zhang's algorithm.  Given a set of world coordinates \(X_i\) and their observed pixel locations \(x_i\), the intrinsic matrix \(K\) and distortion parameters are solved by minimizing

$$
\sum_i \left\| x_i - \pi( K [R \; t] X_i ) \right\|^2,
$$

where $\pi$ is the perspective projection.  Handâ€‘eye calibration solves the classic $AX = XB$ equation using multiple poses from the robot and the target marker. The unknown transform $X$ (camera with respect to tool) is recovered via methods such as Tsaiâ€“Lenz or Daniilidis. Once $X$ is known, point clouds can be transformed to the robot base with

$$
T_{base \leftarrow cam} = T_{base \leftarrow tcp} \times T_{tcp \leftarrow tool} \times T_{tool \leftarrow cam}.
$$

Rigid motions are represented as 4Ã—4 matrices in $SE(3)$. Composition uses matrix multiplication and inverse transforms use the transpose for rotation with the negative translated vector.

### CLI Tools

Entry points defined in `pyproject.toml` expose the common workflows:
`poses-saver`, `path-runner`, `charuco-calib`, etc. The underlying logic lives within the respective packages.

### Extensibility/Testing
* Logger, config, robot, camera: all support dependency injection for unit tests or swapping implementations.
* Add new data savers, control strategies, vision pipelines, or point cloud processors with minimal edits.

---

## ðŸ§° Troubleshooting

* All logs are stored in `logs/` (JSON if enabled)
* Use `logging.level` and `logging.json` in config.yaml to control verbosity/format
* CLI tools print progress, errors, and file names
* For RealSense errors: check camera connection, permissions, drivers, and stream config
* For robot errors: verify IP, cable, firewall, and physical enable state
* For point cloud issues: check calibration files, cloud parameters, and output formats

---

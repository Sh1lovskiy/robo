# Robotics Vision & Calibration Suite

A modular, production-grade Python toolkit for robotics calibration, vision, 3D point cloud processing, and automated pose/trajectory collection.

This repository groups several focused modules under a single project umbrella. Each individual components remain testable and extensible. The sections below describe the directory layout and how the pieces fit together.

**Supports:**

* Robot high-level API (move, home, restart, state, RPC)
* Pluggable camera interface with a RealSense implementation
* Automated Charuco and hand-eye calibration (OpenCV)
* 3D point cloud capture, filtering and visualization
* CLI tools built with a `CommandDispatcher` for calibration, robot control and cloud ops
* Centralized logger and `ErrorTracker` with optional JSON output
* RocksDB-based storage backend for poses, images and metrics
* LMDB-based storage backend for poses, images and metrics

---

## ðŸ“‚ Project Structure (Navigator)

```
project-root/
â”‚
â”œâ”€â”€ calibration/          # Calibration algorithms & workflows
â”‚   â”œâ”€â”€ charuco.py              # Charuco board calibration helpers
â”‚   â”œâ”€â”€ handeye.py              # Hand-eye calibration helpers
â”‚   â”œâ”€â”€ pose_loader.py          # Load robot poses from JSON
â”‚   â”œâ”€â”€ workflows.py            # High-level calibration routines & CLI
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ robot/                # Robot API & workflows
â”‚   â”œâ”€â”€ controller.py           # Main control class
â”‚   â”œâ”€â”€ workflows.py            # Pose recorder and path runner
â”‚   â”œâ”€â”€ marker.py               # Simple marker utilities
â”‚   â”œâ”€â”€ Robot.py                # Cython RPC bindings
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ utils/                # Common utilities
â”‚   â”œâ”€â”€ config.py               # Config loading/abstraction
â”‚   â”œâ”€â”€ logger.py               # Centralized, JSON-capable logger
â”‚   â”œâ”€â”€ error_tracker.py        # Global exception and signal handling
â”‚   â”œâ”€â”€ cli.py                  # CommandDispatcher helper
â”‚   â”œâ”€â”€ keyboard.py             # Global hotkey listener
â”‚   â”œâ”€â”€ io.py                   # Camera calibration I/O
â”‚   â”œâ”€â”€ geometry.py             # Math helpers
â”‚   â””â”€â”€ README.md               # Package overview
â”‚
â”œâ”€â”€ storage/             # LMDB storage layer
â”‚   â”œâ”€â”€ interface.py          # IStorage definitions
â”‚   â”œâ”€â”€ lmdb_storage.py       # LMDB implementation
â”‚   â””â”€â”€ README.md             # Package overview
â”‚
â”œâ”€â”€ vision/               # Vision, cloud, and camera utils
â”‚   â”œâ”€â”€ camera_base.py          # Abstract camera interface
â”‚   â”œâ”€â”€ realsense.py            # RealSense camera implementation
â”‚   â”œâ”€â”€ camera_utils.py         # Depth/intrinsic debug helpers
â”‚   â”œâ”€â”€ opencv_utils.py         # OpenCV helper class
â”‚   â”œâ”€â”€ cloud/                  # Point cloud subpackage
â”‚   â”‚   â”œâ”€â”€ generator.py            # PointCloudGenerator class
â”‚   â”‚   â”œâ”€â”€ aggregator.py           # Multi-frame cloud builder
â”‚   â”‚   â””â”€â”€ pipeline.py             # Filtering/analysis helpers
â”‚   â”œâ”€â”€ tools.py                # Camera and cloud helper CLI
â”‚   â”œâ”€â”€ transform.py            # 3D transformation utilities
â”‚   â””â”€â”€ README.md               # Explanation of transform chain
â”‚
â”œâ”€â”€ conf/app.yaml         # Main configuration file
â”œâ”€â”€ pyproject.toml        # Project metadata & dependencies
â””â”€â”€ README.md             # You are here
```

---

## Overview

The project is organized into four main packages that mirror typical robotics layers:

* **calibration/** â€“ Charuco board detection and handâ€‘eye calibration. The mathematical background of the pose estimation and the classical `AX = XB` formulation are explained in [calibration/README.md](calibration/README.md).
* **robot/** â€“ Robot connection logic and highâ€‘level motion API. Communication is decoupled from workflows so hardware can be replaced without touching the algorithm code (Dependency Inversion Principle).
* **vision/** â€“ Camera interfaces, point cloud tools, and coordinate transforms. Transform chains and coordinate conventions appear in [vision/README.md](vision/README.md).
* **utils/** â€“ Shared helpers for configuration, logging and geometry calculations.
* **storage/** â€“ LMDB based persistence layer for images and metadata.

Every component keeps a single responsibility and exposes a minimal interface. New robots or cameras can be integrated by implementing the same interfaces without modifying existing modules.

### Design Principles

* **Single Responsibility** â€“ each module focuses on one task only.
* **Open/Closed** â€“ functionality is extended via abstractions (e.g. `Camera` base class).
* **Liskov Substitution** â€“ alternate controllers or cameras can drop in without breaking workflows.
* **Interface Segregation** â€“ small APIs are preferred for ease of testing.
* **Dependency Inversion** â€“ high level workflows depend on abstract interfaces, not implementations.

---

## ðŸš€ Quickstart

### 1. Build & Install

Use `pyproject.toml` together with [uv](https://github.com/astral-sh/uv) for reproducible environments/builds:

```bash
uv venv .venv -p 3.12
uv pip install -e .
```

All dependencies are defined in `pyproject.toml`.

### 2. Configure

Edit `conf/app.yaml` for your robot/camera IP, tool, velocity, logging, and point cloud settings. Configuration is managed with [Hydra](https://github.com/facebookresearch/hydra), so any value can be overridden via the command line:

```bash
python some_module.py robot.ip=192.168.1.10 logging.level=DEBUG
```

### 3. Run CLI Tools
CLI modules are thin wrappers calling workflow helpers under
`calibration/`, `robot/`, and `vision/`.

* Save a pose:

  ```bash
  poses-saver
  ```
* Run a trajectory from file:

  ```bash
  path-runner
  ```
* Calibrate camera (Charuco):

  ```bash
  charuco-calib
  ```
* Capture point cloud:

  ```bash
  pointcloud-capture --output clouds/cloud.ply
  ```

---

## ðŸ“‘ Documentation

### Configuration (`conf/app.yaml`)

* `robot:` â€” IP, tool/user frame, velocity, emergency delay
* `vision:` â€” RealSense stream parameters, cloud parameters
* `logging:` â€” log directory, level, JSON output
* `cloud:` â€” point cloud setting, (resolution, voxel size, output dir, ...)
* Default paths, robot IP and Charuco dictionary mapping live in `conf/app.yaml`

### Logger (`utils/logger.py`)

* Central logger: file/console, JSON/text, progress bars
* Decorators for function/exception logging
* Integrated `ErrorTracker` for uncaught exceptions
* Use in all modules:

  ```python
  from utils.logger import Logger
  logger = Logger.get_logger("my.module", json_format=True)
  ```

### Error Tracker (`utils/error_tracker.py`)

* Global exception and signal handler
* Executes registered cleanup functions on failures
* Optional hotkeys via `utils.keyboard`

### Storage (`storage/lmdb_storage.py`)

* `LmdbStorage` â€” default persistence backend implementing `IStorage`

```python
from storage import LmdbStorage
store = LmdbStorage("app.lmdb")
store.put_json("pose:0", {"tcp_coords": [0, 0, 0, 0, 0, 0]})
img = store.get_image("rgb:0")
```

### Robot API (`robot/controller.py`)

* `RobotController` â€” High-level API: connect, move, restart, record, return home, shutdown
* Uses `Config` and `Logger` instances

### Calibration Modules

* `CharucoCalibrator` â€” Board-based camera calibration (OpenCV ArUco)
* `HandEyeCalibrator` â€” Tsai/Daniilidis methods, modular savers for results

### Vision

* `Camera` base class â€” abstract interface for camera drivers
* `RealSenseCamera` â€” Start/stop, get frames, intrinsics, depth scale
* `camera_utils.py` â€” intrinsics printer and depth checker
* `opencv_utils.py` â€” draw\_text, normalize\_depth, apply\_colormap

### 3D Point Cloud Modules

* `cloud/generator.py` â€” Depth to point cloud conversion, save/load (PLY, XYZ, npz)
* `cloud/aggregator.py` â€” Multi-frame cloud assembly with optional ICP
* `cloud/pipeline.py` â€” Filtering, clustering, and trajectory analysis
* `transform.py` â€” Rigid transformations between camera/robot/world coordinates (uses calibration, see vision/README.md)
* CLI scripts provided via entry points (`pointcloud-capture`, `pointcloud-transform`, `pointcloud-view`) and built with `utils.cli.CommandDispatcher`

**See [vision/README.md](./vision/README.md) for detailed usage, coordinate system details, and all math for 3D transforms.**

**Typical workflow:**

1. Capture depth + color â†’ generate cloud (`pointcloud-capture`)
2. (Optional) Filter, merge, or transform clouds (robot <-> camera <-> world)
3. Save or visualize result (`pointcloud-view`)

### Hand-Eye Validation

Run `validation-cli` to compute projection errors and analyze datasets. Example:

```bash
validation-cli validate-pose
validation-cli analyze-dataset
```

## Mathematical Background

Camera calibration uses OpenCV's implementation of Zhang's algorithm.  Given a set of world coordinates \(X_i\) and their observed pixel locations \(x_i\), the intrinsic matrix \(K\) and distortion parameters are solved by minimizing

$$
\sum_i \left\| x_i - \pi( K [R \; t] X_i ) \right\|^2,
$$

where $\pi$ is the perspective projection.  Handâ€‘eye calibration solves the classic $AX = XB$ equation using multiple poses from the robot and the target marker. The unknown transform $X$ (camera with respect to tool) is recovered via methods such as Tsaiâ€“Lenz or Daniilidis. Once $X$ is known, point clouds can be transformed to the robot base with

$$
T_{base \leftarrow cam} = T_{base \leftarrow tcp} \times T_{tcp \leftarrow tool} \times T_{tool \leftarrow cam}.
$$

Rigid motions are represented as 4Ã—4 matrices in $SE(3)$. Composition uses matrix multiplication and inverse transforms use the transpose for rotation with the negative translated vector.

### CLI Tools

Entry points defined in `pyproject.toml` expose the common workflows:
`poses-saver`, `path-runner`, `charuco-calib`, etc. The underlying logic lives within the respective packages.

### Extensibility/Testing
* Logger, config, robot, camera: all support dependency injection for unit tests or swapping implementations.
* Add new data savers, control strategies, vision pipelines, or point cloud processors with minimal edits.

---

## ðŸ§° Troubleshooting

* All logs are stored in `logs/` (JSON if enabled)
* Use `logging.level` and `logging.json` in `conf/app.yaml` to control verbosity/format
* CLI tools print progress, errors, and file names
* For RealSense errors: check camera connection, permissions, drivers, and stream config
* For robot errors: verify IP, cable, firewall, and physical enable state
* For point cloud issues: check calibration files, cloud parameters, and output formats

---

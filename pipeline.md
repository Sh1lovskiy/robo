# Калибровочный пайплайн для `handeye_chess.py` и `handeye_charuco.py`

Описание пейплайна двух модулей: `handeye_chess.py` (классическая Chess board) и `handeye_charuco.py` (Charuco board). Оба модуля реализуют полный цикл геометрической калибровки RGB-D камеры и hand-eye трансформации для задач робототехники.

---

## Структура

  1. Загрузка параметров камеры и поз робота
  2. Детекция и обработка шаблона на изображениях
  3. Фильтрация по качеству
  4. Оценка позы шаблона через solvePnP
  5. Сборка пар (робот-TCP ↔ шаблон)
  6. Hand-eye калибровка (5 методов)
  7. Сохранение результатов и логов

---

## 1. Загрузка параметров и исходных данных

* **Камера-интринсики:**

  * Матрица внутренних параметров (intrinsics) $K$ (3×3), из файла `data/params/cam_params.yml`.
  * Коэффициенты дисторсии $d$.
* **Экстринсики RGB↔Depth:**

  * Внешние параметры между сенсорами $R\_{ext}$, $t\_{ext}$.
* **Датасет:**

  * RGB-изображения: `frame_*.png` (`uint8`, BGR, пиксели)
  * Карты глубины: `frame_*.npy` (`uint16`, далее перевод в `float`, метры на этапе сохранения фрейма)
  * Позиции TCP-робота: `poses_*.json`, структура `{"tcp_coords": [x, y, z, Rx, Ry, Rz]}`

    * $x, y, z$ - положение TCP (метры)
    * $Rx, Ry, Rz$ - углы Эйлера в градусах (XYZ)

---

## 2. Детекция калибровочного шаблона

* **Chess доска:**

  * Поиск углов через `cv2.findChessboardCorners`.
  * Уточнение позиций углов функцией `cv2.cornerSubPix`.
* **Charuco доска:**

  * Детекция ArUco-маркеров: `cv2.aruco.detectMarkers`.
  * Интерполяция Charuco-углов: `cv2.aruco.interpolateCornersCharuco`.
  * Ассоциация найденных id маркеров с 3D-координатами шаблона.
* **Логи:**

  * Количество детектированных углов/маркеров по каждому кадру
  * Флаг успешной/неуспешной детекции

---

## 3. 3D-реконструкция

* **Преобразование 2D→3D (pinhole-модель):**

  * $$
    X_{cam} = Z \cdot K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}
    $$

    * $K$ - intrinsics, $Z$ - глубина, $(u, v)$ - координаты в px
* **Извлечение глубины:** (учитывается на этапе сохранения фрейма)

  * Карта глубины (depth map) из `.npy`, конвертация:

    * $Z = D \cdot \text{depth scale}$
    * $D$ - глубина (`uint16`), $\text{depth scale}$ - калибровочный коэффициент (0.001)
* **Коррекция по экстринсикам:**

  * $$
    X\_{rgb} = R\_{ext} X\_{depth} + t\_{ext}
    $$

---

## 4. Определение позы шаблона

* Для каждого кадра, где успешно найдены углы/маркеры:

  * Задача Perspective-n-Point (PnP):

    * $rvec, tvec = \text{solvePnP}(X\_{obj}, x\_{img}, K, d)$
    * $X\_{obj}$ - 3D-точки шаблона (метры)
    * $x\_{img}$ - 2D-координаты углов (px)
  * Получение SE(3)-матрицы шаблона относительно камеры
  * Фильтрация по ошибке репроекции (< 1.5 px) и детерминанте вращения (>|0.8|)

---

## 5. Hand-eye калибровка

* **Подготовка пар:**

  * ${T\_{robot}^{(i)}}$ - SE(3) матрицы TCP-робота
  * ${T\_{target}^{(i)}}$ - SE(3) матрицы шаблона
* **Калибровка методами OpenCV:**

  * Tsai, Park, Horaud, Andreff, Daniilidis (`cv2.calibrateHandEye`)
* **Результат:**

  * Вектор трансляции $t\_{he}$ (м)
  * Матрица поворота $R\_{he}$ или углы Эйлера (градусы)

---

## 6. Оценка качества
  * Средняя репроекционная ошибка
  * Количество успешно обработанных кадров
  * Детальные логи по каждому снимку
  * Overlays (PNG) с детектированными углами/маркерами
---

## Форматы входных и выходных данных

* **Входные:**

  * `data/params/cam_params.yml` - параметры камеры
  * `calib/imgs/frame_*.png` - изображения
  * `calib/imgs/frame_*.npy` - глубина
  * `calib/poses_*.json` - TCP-позы
* **Выходные:**

  * `handeye_res.yaml` - hand-eye трансформации и статистика
  * `over/overlay_*.png` - визуализация

---

## Eдиницы измерения

* $K$ - камера-интринсики ($3\times3$)
* $d$ - коэффициенты дисторсии
* $Z$ - глубина (м)
* $X\_{cam}$ - 3D-координаты в системе камеры (м)
* $rvec, tvec$ - параметры solvePnP (углы - рад/град, трансляция - м)
* $R\_{he}, t\_{he}$ - итоговая hand-eye трансформация (матрица вращения + вектор переноса)
* Все результаты hand-eye в метрах и градусах (Euler)

---

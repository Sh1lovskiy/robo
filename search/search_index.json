{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Robotics Vision &amp; Calibration Suite","text":"<p>Welcome to the project documentation. The repository groups several robotics and computer vision utilities under a single umbrella. The core packages are:</p> <ul> <li><code>calibration/</code> \u2013 algorithms for intrinsic camera calibration, hand\u2011eye estimation and data collection.</li> <li><code>robot/</code> \u2013 a minimal robot control API with pose recording workflows.</li> <li><code>vision/</code> \u2013 camera drivers, point cloud utilities and RGB\u2011D pipelines.</li> <li><code>utils/</code> \u2013 shared helpers for logging, CLI dispatch and geometry math.</li> </ul> <p>The README describes the directory layout in detail. Below is an excerpt:</p> <pre><code>project-root/\n\u2502\n\u251c\u2500\u2500 calibration/          # Calibration algorithms &amp; workflows\n\u2502   \u251c\u2500\u2500 pattern.py              # Unified calibration patterns\n\u2502   \u251c\u2500\u2500 handeye.py              # Hand-eye calibration helpers\n\u2502   \u251c\u2500\u2500 pose_loader.py          # Load robot poses from JSON\n\u2502   \u251c\u2500\u2500 workflows.py            # High-level calibration routines &amp; CLI\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 robot/                # Robot API &amp; workflows\n\u2502   \u251c\u2500\u2500 controller.py           # Main control class\n\u2502   \u251c\u2500\u2500 workflows.py            # Pose recorder and path runner\n\u2502   \u251c\u2500\u2500 marker.py               # Simple marker utilities\n\u2502   \u251c\u2500\u2500 Robot.py                # Cython RPC bindings\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 utils/                # Common utilities and storage\n\u2502   \u251c\u2500\u2500 logger.py               # Centralized, JSON-capable logger\n\u2502   \u251c\u2500\u2500 error_tracker.py        # Global exception and signal handling\n\u2502   \u251c\u2500\u2500 cli.py                  # CommandDispatcher helper\n\u2502   \u251c\u2500\u2500 keyboard.py             # Global hotkey listener\n\u2502   \u251c\u2500\u2500 io.py                   # File and camera parameter I/O\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 vision/               # Vision, cloud, and camera utils\n\u2502   \u251c\u2500\u2500 camera_base.py          # Abstract camera interface\n\u2502   \u251c\u2500\u2500 realsense.py            # RealSense camera implementation\n\u2502   \u251c\u2500\u2500 camera_utils.py         # Depth/intrinsic debug helpers\n\u2502   \u251c\u2500\u2500 opencv_utils.py         # OpenCV helper class\n\u2502   \u251c\u2500\u2500 cloud/                  # Point cloud subpackage\n\u2502   \u2502   \u251c\u2500\u2500 generator.py            # PointCloudGenerator class\n\u2502   \u2502   \u251c\u2500\u2500 aggregator.py           # Multi-frame cloud builder\n\u2502   \u2502   \u2514\u2500\u2500 pipeline.py             # Filtering/analysis helpers\n\u2502   \u251c\u2500\u2500 mapping/               # PatchMatch &amp; point set registration\n\u2502   \u251c\u2500\u2500 tools.py                # Camera and cloud helper CLI\n\u2502   \u251c\u2500\u2500 transform.py            # 3D transformation utilities\n\u2502   \u2514\u2500\u2500 README.md               # Explanation of transform chain\n</code></pre> <p>Use the navigation on the left to explore usage examples, pipeline descriptions and the full Python API.</p>"},{"location":"api/","title":"API Reference","text":"<p>The full Python API is generated from type\u2011hints and docstrings using mkdocstrings. Use the module links below to explore each package.</p>"},{"location":"api/#packages","title":"Packages","text":"<ul> <li><code>calibration</code></li> <li><code>robot</code></li> <li><code>vision</code></li> <li><code>utils</code></li> </ul> <p>Robotics calibration package.</p> <p>Camera interfaces and 3D vision helpers.</p> <p>The vision package contains camera drivers, point cloud tools and transformation utilities used throughout the robotics stack.  Open3D and OpenCV are leveraged to convert depth images to point clouds, apply filters and perform coordinate transforms.  Subpackages provide reusable components that can be combined in custom pipelines or accessed through the provided CLI tools.</p> <p>Shared helper modules used across the project.</p> <p>The :mod:<code>utils</code> package contains lightweight helpers for logging, CLI dispatching and simple file I/O. These utilities are used by most other packages and avoid additional dependencies.</p>"},{"location":"api/#calibration.HandEyeComparison","title":"<code>HandEyeComparison</code>  <code>dataclass</code>","text":"<p>Run hand-eye calibration for several pose estimation variants.</p> Source code in <code>calibration/comparison.py</code> <pre><code>@dataclass\nclass HandEyeComparison:\n    \"\"\"Run hand-eye calibration for several pose estimation variants.\"\"\"\n\n    visualize: bool = False\n    logger: LoggerType = field(\n        default_factory=lambda: Logger.get_logger(\"calibration.comparison\")\n    )\n\n    def _pose_pnp(\n        self, obj: np.ndarray, img: np.ndarray, K: np.ndarray, dist: np.ndarray\n    ) -&gt; tuple[np.ndarray | None, np.ndarray | None]:\n        if obj.shape[0] &lt; 6:\n            self.logger.warning(f\"Skipping PnP: only {obj.shape[0]} correspondences\")\n            return None, None\n        return solve_pnp_obj_to_img(obj, img, K, dist)\n\n    def _pose_svd(\n        self, model: np.ndarray, meas: np.ndarray\n    ) -&gt; tuple[np.ndarray, np.ndarray] | None:\n        \"\"\"Estimate pose by aligning two 3-D point sets.\"\"\"\n        try:\n            return rigid_transform_3D(model, meas)\n        except Exception as exc:  # pragma: no cover - numerical issues\n            self.logger.warning(f\"SVD failed: {exc}\")\n            return None\n\n    def _depth_points(\n        self,\n        corners: np.ndarray,\n        depth: np.ndarray,\n        K_rgb: np.ndarray,\n        K_depth: np.ndarray,\n    ) -&gt; np.ndarray:\n        R_c2d = np.asarray(EXTR_COLOR_TO_DEPTH_ROT, dtype=np.float64)\n        t_c2d = np.asarray(EXTR_COLOR_TO_DEPTH_TRANS, dtype=np.float64)\n        R_d2c = np.asarray(EXTR_DEPTH_TO_COLOR_ROT, dtype=np.float64)\n        t_d2c = np.asarray(EXTR_DEPTH_TO_COLOR_TRANS, dtype=np.float64)\n        self.logger.debug(\n            f\"[_depth_points] Input: corners shape={corners.shape}, depth shape={depth.shape}\"\n        )\n        self.logger.debug(f\"[_depth_points] K_rgb:\\n{K_rgb}\")\n        self.logger.debug(f\"[_depth_points] K_depth:\\n{K_depth}\")\n        self.logger.debug(f\"[_depth_points] Extrinsics R_d2c:\\n{R_d2c}\\n t_d2c:{t_d2c}\")\n        pts_3d = get_3d_points_from_depth(\n            corners,\n            depth,\n            K_rgb,\n            K_depth,\n            R_c2d,\n            R_d2c,\n            t_c2d,\n            t_d2c,\n            depth_scale=DEPTH_SCALE,\n            logger=self.logger,\n        )\n        self.logger.debug(\n            f\"[_depth_points] Output: 3D points shape={pts_3d.shape}, \"\n            f\"sample: {pts_3d[:2]}\"\n        )\n        return pts_3d\n\n    def _camera_poses(\n        self,\n        robot_Rs: List[np.ndarray],\n        robot_ts: List[np.ndarray],\n        R: np.ndarray,\n        t: np.ndarray,\n        invert: bool = False,\n    ) -&gt; tuple[List[np.ndarray], List[np.ndarray]]:\n        cam_Rs, cam_ts = [], []\n        T_tool = TransformUtils.build_transform(R, t)\n        T_inv = np.linalg.inv(T_tool)\n        for Rg, tg in zip(robot_Rs, robot_ts):\n            T_base = TransformUtils.build_transform(Rg, tg)\n            T = T_tool @ T_base if invert else T_base @ T_inv\n            cam_Rs.append(T[:3, :3])\n            cam_ts.append(T[:3, 3])\n        return cam_Rs, cam_ts\n\n    def _run_method(\n        self,\n        method: int,\n        name: str,\n        robot_Rs: List[np.ndarray],\n        robot_ts: List[np.ndarray],\n        target_Rs: List[np.ndarray],\n        target_ts: List[np.ndarray],\n        out_base: Path,\n    ) -&gt; tuple[str, float, float, float, float]:\n        if len(robot_Rs) &lt; 3 or len(target_Rs) &lt; 3:\n            self.logger.warning(\n                f\"Skipping {name} \u2014 too few pose pairs: {len(robot_Rs)}\"\n            )\n            return name, np.nan, np.nan, np.nan, np.nan\n\n        for i, R in enumerate(target_Rs):\n            det = np.linalg.det(R)\n            if abs(det) &lt; 1e-4:\n                self.logger.warning(\n                    f\"target_R[{i}] has near-zero determinant: {det:.6f}\"\n                )\n\n        with CaptureStderrToLogger(self.logger):\n            try:\n                R_ct, t_ct = cv2.calibrateHandEye(\n                    robot_Rs, robot_ts, target_Rs, target_ts, method=method\n                )\n            except cv2.error as e:\n                self.logger.warning(f\"{name} failed: {e}\")\n                return name, np.nan, np.nan, np.nan, np.nan  # gracefully skip\n\n        out_base.mkdir(parents=True, exist_ok=True)\n        save_transform(out_base / name, TransformUtils.build_transform(R_ct, t_ct))\n        cam_Rs, cam_ts = self._camera_poses(robot_Rs, robot_ts, R_ct, t_ct)\n        robot_Rss, robot_tss = self._camera_poses(robot_Rs, robot_ts, R_ct, t_ct, True)\n\n        rot_err, trans_err = handeye_errors(\n            robot_Rs, robot_ts, target_Rs, target_ts, R_ct, t_ct\n        )\n        t_rmse = float(np.sqrt(np.mean(trans_err**2)))\n        r_rmse = float(np.sqrt(np.mean(rot_err**2)))\n        trans = np.linalg.norm(np.array(robot_tss) - np.array(cam_ts), axis=1)\n        rot = [rotation_angle(Rr.T @ Rc) for Rr, Rc in zip(robot_Rss, cam_Rs)]\n        a_t_rmse = float(np.sqrt(np.mean(trans**2)))\n        a_r_rmse = float(np.sqrt(np.mean(np.square(rot))))\n\n        if self.visualize:\n            plot_file = paths.VIZ_DIR / f\"{out_base.stem}_{name}_poses.png\"\n            plot_poses(robot_Rs, robot_ts, cam_Rs, cam_ts, plot_file)\n\n        return name, t_rmse, r_rmse, a_t_rmse, a_r_rmse\n\n    def _log_summary(\n        self, summary: List[tuple[str, float, float, float, float]], out_dir: Path\n    ) -&gt; None:\n        lines = [\n            \"Summary of hand-eye calibration and pose alignment errors:\",\n            \"Method     | T. RMSE [m] | R. RMSE [deg] | Align T. RMSE [m] | Align R. RMSE [deg]\",\n            \"-----------|-------------|---------------|-------------------|-------------------\",\n        ]\n        for n, t1, r1, t2, r2 in summary:\n            lines.append(\n                f\"{n:&lt;10} |  {t1:&gt;9.6f}  |  {r1:&gt;11.4f}  |  {t2:&gt;15.6f}  |  {r2:&gt;17.4f}\"\n            )\n        lines.append(f\"Results saved to {out_dir.relative_to(Path.cwd())}\")\n        self.logger.info(\"\\n\".join(lines))\n\n    def _collect(\n        self,\n        images: List[Path],\n        pattern: CalibrationPattern,\n        intrinsics: tuple[np.ndarray, np.ndarray],\n        robot_Rs: List[np.ndarray],\n        robot_ts: List[np.ndarray],\n    ) -&gt; Dict[str, Dict[str, List[np.ndarray]]]:\n        K_rgb = np.asarray(INTRINSICS_COLOR_MATRIX, dtype=np.float64)\n        dist = np.array(\n            [\n                0.14276977476919803,\n                -0.37253190781644513,\n                -0.002400175306122351,\n                0.005829250084910286,\n                0.23582308984644337,\n            ]\n        )\n        K_depth = np.asarray(INTRINSICS_DEPTH_MATRIX, dtype=np.float64)\n        variants = {\n            \"pnp\": {\"robot_Rs\": [], \"robot_ts\": [], \"R\": [], \"t\": []},\n            \"svd\": {\"robot_Rs\": [], \"robot_ts\": [], \"R\": [], \"t\": []},\n            \"pnp_depth\": {\"robot_Rs\": [], \"robot_ts\": [], \"R\": [], \"t\": []},\n        }\n        for idx, img_path in enumerate(Logger.progress(images, desc=\"detect\")):\n            if idx &gt;= len(robot_Rs):\n                break\n            img = cv2.imread(str(img_path))\n            if img is None:\n                continue\n            detection = pattern.detect(img, visualize=False)\n            if detection is None:\n                continue\n            corners = detection.corners  # Nx2\n            object_points = detection.object_points\n            depth_file = img_path.with_suffix(DEPTH_EXT)\n            if not depth_file.exists():\n                continue\n            depth = load_depth(str(depth_file))\n            # get 3d points, order the same as detection.corners/object_points\n            depth_pts = self._depth_points(detection.corners, depth, K_rgb, K_depth)\n            mask = ~np.isnan(depth_pts).any(axis=1)\n            if np.sum(mask) &lt; 4:\n                continue\n            # align pairs\n            corners_valid = corners[mask]  # Nx2\n            object_points_valid = object_points[mask]  # Nx3\n            depth_pts_valid = depth_pts[mask]  # Nx3\n            self.logger.debug(\n                f\"[{img_path.name}] First valid point: corner RGB={corners_valid[0]}, \"\n                f\"object_point={object_points_valid[0]}, depth_3d={depth_pts_valid[0]}\"\n            )\n            if (\n                corners_valid.shape[0] != object_points_valid.shape[0]\n                or corners_valid.shape[0] != depth_pts_valid.shape[0]\n            ):\n                self.logger.warning(\"Corner/depth correspondence mismatch\")\n                continue\n            self.logger.debug(\n                f\"[{img_path.name}] valid points: {corners_valid.shape[0]}\"\n            )\n\n            # 2D-3D PnP\n            R_pnp2d, t_pnp2d = self._pose_pnp(\n                object_points_valid, corners_valid, K_rgb, dist\n            )\n            if R_pnp2d is not None and t_pnp2d is not None:\n                object_points_in_rgb = (\n                    R_pnp2d @ object_points_valid.T\n                ).T + t_pnp2d.flatten()\n                self.logger.debug(\n                    f\"[PnP] Transformed first object point to RGB: {object_points_in_rgb[0]}\"\n                )\n                variants[\"pnp\"][\"robot_Rs\"].append(robot_Rs[idx])\n                variants[\"pnp\"][\"robot_ts\"].append(robot_ts[idx])\n                variants[\"pnp\"][\"R\"].append(R_pnp2d)\n                variants[\"pnp\"][\"t\"].append(t_pnp2d)\n\n                # SVD 3D-3D: object_points_in_rgb to depth_pts_valid with mask\n                pose_svd = rigid_transform_3D(object_points_valid, depth_pts_valid)\n                if (\n                    pose_svd is not None\n                    and pose_svd[0] is not None\n                    and pose_svd[1] is not None\n                ):\n                    variants[\"svd\"][\"robot_Rs\"].append(robot_Rs[idx])\n                    variants[\"svd\"][\"robot_ts\"].append(robot_ts[idx])\n                    variants[\"svd\"][\"R\"].append(pose_svd[0])\n                    variants[\"svd\"][\"t\"].append(pose_svd[1])\n\n                pose_pd = solve_pnp_obj_to_3d(depth_pts_valid, object_points_valid)\n                if (\n                    pose_pd is not None\n                    and pose_pd[0] is not None\n                    and pose_pd[1] is not None\n                ):\n                    variants[\"pnp_depth\"][\"robot_Rs\"].append(robot_Rs[idx])\n                    variants[\"pnp_depth\"][\"robot_ts\"].append(robot_ts[idx])\n                    variants[\"pnp_depth\"][\"R\"].append(pose_pd[0])\n                    variants[\"pnp_depth\"][\"t\"].append(pose_pd[1])\n        return variants\n\n    def calibrate(\n        self,\n        poses_file: Path,\n        images: List[Path],\n        pattern: CalibrationPattern,\n        intrinsics: tuple[np.ndarray, np.ndarray],\n    ) -&gt; None:\n        self.logger.info(\"Starting hand-eye comparison\")\n        try:\n            robot_Rs, robot_ts = JSONPoseLoader.load_poses(str(poses_file))\n            data = self._collect(images, pattern, intrinsics, robot_Rs, robot_ts)\n            for variant, vals in data.items():\n                if not vals[\"R\"]:\n                    self.logger.warning(f\"No valid poses for {variant}\")\n                    continue\n                out_base = paths.RESULTS_DIR / f\"handeye_{variant}_{timestamp()}\"\n                summary = []\n                for m, name in HAND_EYE_METHODS[:-1]:\n                    self.logger.info(f\"Method {name.upper()} on {variant}\")\n                    res = self._run_method(\n                        m,\n                        name,\n                        vals[\"robot_Rs\"],\n                        vals[\"robot_ts\"],\n                        vals[\"R\"],\n                        vals[\"t\"],\n                        out_base,\n                    )\n                    summary.append(res)\n                self._log_summary(summary, out_base)\n        except Exception as exc:\n            self.logger.error(f\"Hand-eye comparison failed: {exc}\")\n            ErrorTracker.report(exc)\n        finally:\n            pattern.clear()\n</code></pre>"},{"location":"api/#vision.CameraBase","title":"<code>CameraBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Minimal camera control API.</p> Source code in <code>vision/camera/camera_base.py</code> <pre><code>class CameraBase(ABC):\n    \"\"\"Minimal camera control API.\"\"\"\n\n    @abstractmethod\n    def start(self) -&gt; None:\n        \"\"\"Start streaming.\"\"\"\n\n    @abstractmethod\n    def stop(self) -&gt; None:\n        \"\"\"Stop streaming.\"\"\"\n\n    @abstractmethod\n    def get_frames(\n        self, aligned: bool = True\n    ) -&gt; Tuple[np.ndarray | None, np.ndarray | None]:\n        \"\"\"Return color and depth frames.\"\"\"\n</code></pre>"},{"location":"api/#vision.CameraBase.get_frames","title":"<code>get_frames(aligned=True)</code>  <code>abstractmethod</code>","text":"<p>Return color and depth frames.</p> Source code in <code>vision/camera/camera_base.py</code> <pre><code>@abstractmethod\ndef get_frames(\n    self, aligned: bool = True\n) -&gt; Tuple[np.ndarray | None, np.ndarray | None]:\n    \"\"\"Return color and depth frames.\"\"\"\n</code></pre>"},{"location":"api/#vision.CameraBase.start","title":"<code>start()</code>  <code>abstractmethod</code>","text":"<p>Start streaming.</p> Source code in <code>vision/camera/camera_base.py</code> <pre><code>@abstractmethod\ndef start(self) -&gt; None:\n    \"\"\"Start streaming.\"\"\"\n</code></pre>"},{"location":"api/#vision.CameraBase.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Stop streaming.</p> Source code in <code>vision/camera/camera_base.py</code> <pre><code>@abstractmethod\ndef stop(self) -&gt; None:\n    \"\"\"Stop streaming.\"\"\"\n</code></pre>"},{"location":"api/#vision.CloudAnalyzer","title":"<code>CloudAnalyzer</code>","text":"<p>Full visualization and analysis pipeline.</p> Source code in <code>vision/pointcloud/analyzer.py</code> <pre><code>class CloudAnalyzer:\n    \"\"\"Full visualization and analysis pipeline.\"\"\"\n\n    def __init__(self, logger: LoggerType | None = None) -&gt; None:\n        self.logger = logger or Logger.get_logger(\"vision.pipeline\")\n        self.denoiser = PointCloudDenoiser(logger=self.logger)\n        self.cropper = PointCloudCropper(logger=self.logger)\n        self.clusterer = PointCloudClusterer(logger=self.logger)\n        self.analyzer = ObjectSurfaceAnalyzer(logger=self.logger)\n        self.topfinder = TopFaceFinder(logger=self.logger)\n        self.trajectory_planner = TopFaceTrajectoryPlanner(logger=self.logger)\n\n    def run(self, input_ply: str) -&gt; None:\n        pcd = o3d.io.read_point_cloud(input_ply)\n        self.logger.info(f\"Loaded {input_ply}, {len(pcd.points)} points\")\n        o3d.visualization.draw_geometries_with_editing([pcd], window_name=\"Raw cloud\")\n\n        cropped = self.cropper.crop(pcd)\n        # o3d.visualization.draw_geometries_with_editing(\n        #     [cropped], window_name=\"Cropped cloud\"\n        # )\n\n        clean = self.denoiser.denoise(cropped)\n        # o3d.visualization.draw_geometries_with_editing(\n        #     [clean], window_name=\"Denoised cloud\"\n        # )\n\n        obj = self.clusterer.extract_object(clean)\n        # o3d.visualization.draw_geometries([obj], window_name=\"Clustered object\")\n\n        aabb, _ = self.analyzer.get_bounding_box(obj)\n        o3d.visualization.draw_geometries([obj, aabb], window_name=\"Bounding box\")\n\n        top_points = self.topfinder.find_top_face(obj)\n        if len(top_points) &gt; 0:\n            top_pcd = o3d.geometry.PointCloud()\n            top_pcd.points = o3d.utility.Vector3dVector(top_points)\n            top_pcd.paint_uniform_color([1, 1, 0])\n            # o3d.visualization.draw_geometries(\n            #    [obj, top_pcd], window_name=\"Top face points\"\n            # )\n            traj_start, traj_end = self.trajectory_planner.plan_center_trajectory(\n                top_points\n            )\n            traj_line = o3d.geometry.LineSet()\n            traj_line.points = o3d.utility.Vector3dVector([traj_start, traj_end])\n            traj_line.lines = o3d.utility.Vector2iVector([[0, 1]])\n            traj_line.colors = o3d.utility.Vector3dVector([[1, 0, 0]])\n            o3d.visualization.draw_geometries(\n                [obj, top_pcd, traj_line], window_name=\"Marker Trajectory\"\n            )\n</code></pre>"},{"location":"api/#vision.PointCloudBuilder","title":"<code>PointCloudBuilder</code>  <code>dataclass</code>","text":"<p>Capture and process clouds using a calibrated camera.</p> Source code in <code>vision/pointcloud/builder.py</code> <pre><code>@dataclass\nclass PointCloudBuilder:\n    \"\"\"Capture and process clouds using a calibrated camera.\"\"\"\n\n    camera: CameraBase\n    handeye: Tuple[np.ndarray, np.ndarray]\n    logger: LoggerType = field(\n        default_factory=lambda: Logger.get_logger(\"vision.pointcloud.builder\")\n    )\n    transformer: TransformUtils = field(default_factory=TransformUtils)\n    generator: PointCloudGenerator = field(default_factory=PointCloudGenerator)\n\n    def capture(self, robot_pose: Iterable[float]) -&gt; o3d.geometry.PointCloud:\n        color, depth = self.camera.get_frames()\n        stream = self.camera.profile.get_stream(rs.stream.depth)\n        intr = stream.as_video_stream_profile().get_intrinsics()\n        K = {\"fx\": intr.fx, \"fy\": intr.fy, \"ppx\": intr.ppx, \"ppy\": intr.ppy}\n        depth_m = depth.astype(np.float32) * self.camera.depth_scale\n        points, colors = self.generator.depth_to_cloud(depth_m, K, color)\n        Rb, tb = self._robot_pose_to_rt(robot_pose)\n        Rc, tc = self.handeye\n        T = self.transformer.get_base_to_camera((Rb, tb), None, Rc, tc)\n        pts_base = self.transformer.transform_points(points, T)\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(pts_base)\n        if colors is not None:\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n        pcd = pcd.voxel_down_sample(voxel_size=0.002)\n        pcd, _ = pcd.remove_statistical_outlier(20, 1.0)\n        self.logger.info(\n            f\"Cloud: {len(pcd.points)} points, mean Z {np.mean(points[:,2]):.3f} m\"\n        )\n        return pcd\n\n    def _robot_pose_to_rt(self, pose: Iterable[float]) -&gt; Tuple[np.ndarray, np.ndarray]:\n        arr = list(pose)\n        R = euler_to_matrix(arr[3], arr[4], arr[5], degrees=True)\n        t = np.array(arr[:3], dtype=float) / 1000.0\n        return R, t\n\n    def save(self, pcd: o3d.geometry.PointCloud, path: str) -&gt; None:\n        o3d.io.write_point_cloud(path, pcd)\n        self.logger.info(f\"Saved cloud to {path}\")\n\n    def load(self, path: str) -&gt; o3d.geometry.PointCloud:\n        pcd = o3d.io.read_point_cloud(path)\n        self.logger.info(f\"Loaded cloud from {path}\")\n        return pcd\n\n    def remove_plane(\n        self, pcd: o3d.geometry.PointCloud, dist: float = 0.003\n    ) -&gt; o3d.geometry.PointCloud:\n        plane_model, inliers = pcd.segment_plane(dist, 3, 1000)\n        self.logger.debug(\n            f\"Plane: {np.round(plane_model,4).tolist()} with {len(inliers)} inliers\"\n        )\n        return pcd.select_by_index(inliers, invert=True)\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator","title":"<code>PointCloudGenerator</code>","text":"<p>Utility class for creating and handling point clouds.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>class PointCloudGenerator:\n    \"\"\"Utility class for creating and handling point clouds.\"\"\"\n\n    @staticmethod\n    def pose_to_transform(\n        pose: list[float] | np.ndarray,\n        *,\n        angles_in_deg: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Convert [x, y, z, rx, ry, rz] pose to a 4x4 homogeneous transformation matrix.\n\n        Args:\n            pose: 6-DOF pose as [x, y, z, rx, ry, rz].\n                x, y, z in millimeters.\n                rx, ry, rz are Euler angles (degrees or radians).\n            angles_in_deg: Whether the input angles are in degrees (default: True).\n\n        Returns:\n            4x4 SE(3) transformation matrix (numpy.ndarray)\n        \"\"\"\n        pose = np.asarray(pose, dtype=np.float64)\n        if pose.shape != (6,):\n            raise ValueError(\n                \"Pose must be a sequence of 6 floats: [x, y, z, rx, ry, rz]\"\n            )\n\n        position = pose[:3] / 1000.0  # mm \u2192 m\n        rotation = euler_to_matrix(*pose[3:], degrees=angles_in_deg)  # 3x3\n\n        return make_transform(rotation, position)\n\n    @staticmethod\n    def depth_to_cloud(\n        depth: np.ndarray,\n        intr: dict,\n        rgb: np.ndarray | None = None,\n        depth_range: tuple[float, float] = (0.1, 1.0),\n    ) -&gt; tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"\n        Convert a depth map (and optionally an RGB image) to a 3D point cloud.\n\n        Args:\n            depth: HxW float array of depth (meters).\n            intr: Dict with camera intrinsics: fx, fy, ppx (cx), ppy (cy).\n            rgb: Optional HxWx3 or HxWx4 uint8 image for colors.\n            depth_range: Valid depth range (min, max), in meters.\n\n        Returns:\n            points: (N, 3) float32 array of 3D points.\n            colors: (N, 3) float32 array of RGB colors in [0, 1], or None.\n        \"\"\"\n        h, w = depth.shape\n        fx, fy = intr[\"fx\"], intr[\"fy\"]\n        cx, cy = intr[\"ppx\"], intr[\"ppy\"]\n        z_min, z_max = depth_range\n        mask = (depth &gt; z_min) &amp; (depth &lt; z_max)\n        # np.where return y, x\n        ys, xs = np.where(mask)\n        zs = depth[ys, xs]\n        # pinhole formula\n        xs_ = (xs - cx) * zs / fx\n        ys_ = (ys - cy) * zs / fy\n        # stack in cloud Nx3\n        points = np.stack((xs_, ys_, zs), axis=1)\n        colors = None\n        if rgb is not None:\n            rgb = rgb[..., :3]  # Discard alpha if present\n            # rgb[ys, xs] \u2014 colors for valid points\n            colors = rgb[ys, xs]\n            if colors.shape[1] == 3:\n                colors = colors[:, ::-1]\n            colors = colors.astype(np.float32) / 255.0  # [:, ::-1] RGB \u2192 BGR for opencv\n            return points, colors\n        return points, None\n\n    @staticmethod\n    def save_ply(\n        filename: str,\n        points: np.ndarray,\n        colors: np.ndarray | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Save a point cloud to a PLY file using Open3D.\n\n        Args:\n            filename: Output path.\n            points: (N, 3) array.\n            colors: (N, 3) array in [0, 1] or None.\n        \"\"\"\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(points)\n        if colors is not None:\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n        o3d.io.write_point_cloud(filename, pcd)\n\n    @staticmethod\n    def load_ply(filename: str) -&gt; tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"\n        Load a point cloud from a PLY file.\n\n        Returns:\n            points: (N, 3) array.\n            colors: (N, 3) array or None.\n        \"\"\"\n        pcd = o3d.io.read_point_cloud(filename)\n        points = np.asarray(pcd.points)\n        colors = np.asarray(pcd.colors) if pcd.has_colors() else None\n        return points, colors\n\n    @staticmethod\n    def downsample_cloud(\n        pcd: o3d.geometry.PointCloud, voxel_size: float = 0.005\n    ) -&gt; o3d.geometry.PointCloud:\n        \"\"\"\n        Downsample a point cloud using voxel grid filtering.\n        \"\"\"\n        return pcd.voxel_down_sample(voxel_size)\n\n    @staticmethod\n    def icp_pairwise_align(\n        source: o3d.geometry.PointCloud,\n        target: o3d.geometry.PointCloud,\n        threshold: float = 0.02,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Perform pairwise ICP registration (point-to-point).\n\n        Returns:\n            4x4 transformation matrix.\n        \"\"\"\n        reg = o3d.pipelines.registration.registration_icp(\n            source,\n            target,\n            threshold,\n            np.eye(4),\n            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n        )\n        return reg.transformation\n\n    @staticmethod\n    def merge_clouds(\n        clouds: list[o3d.geometry.PointCloud],\n        voxel_size: float = 0.003,\n    ) -&gt; o3d.geometry.PointCloud:\n        \"\"\"\n        Merge a list of point clouds, with optional voxel downsampling.\n        \"\"\"\n        merged = o3d.geometry.PointCloud()\n        for pcd in clouds:\n            merged += pcd\n        if voxel_size:\n            merged = merged.voxel_down_sample(voxel_size)\n        return merged\n\n    @staticmethod\n    def filter_cloud(\n        pcd: o3d.geometry.PointCloud,\n        nb_neighbors: int = 20,\n        std_ratio: float = 2.0,\n    ) -&gt; o3d.geometry.PointCloud:\n        \"\"\"\n        Remove outliers from a point cloud using statistical filtering.\n        \"\"\"\n        cl, _ = pcd.remove_statistical_outlier(\n            nb_neighbors=nb_neighbors, std_ratio=std_ratio\n        )\n        return cl\n\n    @staticmethod\n    def visualize(pcd: o3d.geometry.PointCloud) -&gt; None:\n        \"\"\"\n        Visualize a point cloud using Open3D's viewer.\n        \"\"\"\n        o3d.visualization.draw_geometries([pcd])\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.depth_to_cloud","title":"<code>depth_to_cloud(depth, intr, rgb=None, depth_range=(0.1, 1.0))</code>  <code>staticmethod</code>","text":"<p>Convert a depth map (and optionally an RGB image) to a 3D point cloud.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>ndarray</code> <p>HxW float array of depth (meters).</p> required <code>intr</code> <code>dict</code> <p>Dict with camera intrinsics: fx, fy, ppx (cx), ppy (cy).</p> required <code>rgb</code> <code>ndarray | None</code> <p>Optional HxWx3 or HxWx4 uint8 image for colors.</p> <code>None</code> <code>depth_range</code> <code>tuple[float, float]</code> <p>Valid depth range (min, max), in meters.</p> <code>(0.1, 1.0)</code> <p>Returns:</p> Name Type Description <code>points</code> <code>ndarray</code> <p>(N, 3) float32 array of 3D points.</p> <code>colors</code> <code>ndarray | None</code> <p>(N, 3) float32 array of RGB colors in [0, 1], or None.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef depth_to_cloud(\n    depth: np.ndarray,\n    intr: dict,\n    rgb: np.ndarray | None = None,\n    depth_range: tuple[float, float] = (0.1, 1.0),\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"\n    Convert a depth map (and optionally an RGB image) to a 3D point cloud.\n\n    Args:\n        depth: HxW float array of depth (meters).\n        intr: Dict with camera intrinsics: fx, fy, ppx (cx), ppy (cy).\n        rgb: Optional HxWx3 or HxWx4 uint8 image for colors.\n        depth_range: Valid depth range (min, max), in meters.\n\n    Returns:\n        points: (N, 3) float32 array of 3D points.\n        colors: (N, 3) float32 array of RGB colors in [0, 1], or None.\n    \"\"\"\n    h, w = depth.shape\n    fx, fy = intr[\"fx\"], intr[\"fy\"]\n    cx, cy = intr[\"ppx\"], intr[\"ppy\"]\n    z_min, z_max = depth_range\n    mask = (depth &gt; z_min) &amp; (depth &lt; z_max)\n    # np.where return y, x\n    ys, xs = np.where(mask)\n    zs = depth[ys, xs]\n    # pinhole formula\n    xs_ = (xs - cx) * zs / fx\n    ys_ = (ys - cy) * zs / fy\n    # stack in cloud Nx3\n    points = np.stack((xs_, ys_, zs), axis=1)\n    colors = None\n    if rgb is not None:\n        rgb = rgb[..., :3]  # Discard alpha if present\n        # rgb[ys, xs] \u2014 colors for valid points\n        colors = rgb[ys, xs]\n        if colors.shape[1] == 3:\n            colors = colors[:, ::-1]\n        colors = colors.astype(np.float32) / 255.0  # [:, ::-1] RGB \u2192 BGR for opencv\n        return points, colors\n    return points, None\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.downsample_cloud","title":"<code>downsample_cloud(pcd, voxel_size=0.005)</code>  <code>staticmethod</code>","text":"<p>Downsample a point cloud using voxel grid filtering.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef downsample_cloud(\n    pcd: o3d.geometry.PointCloud, voxel_size: float = 0.005\n) -&gt; o3d.geometry.PointCloud:\n    \"\"\"\n    Downsample a point cloud using voxel grid filtering.\n    \"\"\"\n    return pcd.voxel_down_sample(voxel_size)\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.filter_cloud","title":"<code>filter_cloud(pcd, nb_neighbors=20, std_ratio=2.0)</code>  <code>staticmethod</code>","text":"<p>Remove outliers from a point cloud using statistical filtering.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef filter_cloud(\n    pcd: o3d.geometry.PointCloud,\n    nb_neighbors: int = 20,\n    std_ratio: float = 2.0,\n) -&gt; o3d.geometry.PointCloud:\n    \"\"\"\n    Remove outliers from a point cloud using statistical filtering.\n    \"\"\"\n    cl, _ = pcd.remove_statistical_outlier(\n        nb_neighbors=nb_neighbors, std_ratio=std_ratio\n    )\n    return cl\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.icp_pairwise_align","title":"<code>icp_pairwise_align(source, target, threshold=0.02)</code>  <code>staticmethod</code>","text":"<p>Perform pairwise ICP registration (point-to-point).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>4x4 transformation matrix.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef icp_pairwise_align(\n    source: o3d.geometry.PointCloud,\n    target: o3d.geometry.PointCloud,\n    threshold: float = 0.02,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform pairwise ICP registration (point-to-point).\n\n    Returns:\n        4x4 transformation matrix.\n    \"\"\"\n    reg = o3d.pipelines.registration.registration_icp(\n        source,\n        target,\n        threshold,\n        np.eye(4),\n        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n    )\n    return reg.transformation\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.load_ply","title":"<code>load_ply(filename)</code>  <code>staticmethod</code>","text":"<p>Load a point cloud from a PLY file.</p> <p>Returns:</p> Name Type Description <code>points</code> <code>ndarray</code> <p>(N, 3) array.</p> <code>colors</code> <code>ndarray | None</code> <p>(N, 3) array or None.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef load_ply(filename: str) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"\n    Load a point cloud from a PLY file.\n\n    Returns:\n        points: (N, 3) array.\n        colors: (N, 3) array or None.\n    \"\"\"\n    pcd = o3d.io.read_point_cloud(filename)\n    points = np.asarray(pcd.points)\n    colors = np.asarray(pcd.colors) if pcd.has_colors() else None\n    return points, colors\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.merge_clouds","title":"<code>merge_clouds(clouds, voxel_size=0.003)</code>  <code>staticmethod</code>","text":"<p>Merge a list of point clouds, with optional voxel downsampling.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef merge_clouds(\n    clouds: list[o3d.geometry.PointCloud],\n    voxel_size: float = 0.003,\n) -&gt; o3d.geometry.PointCloud:\n    \"\"\"\n    Merge a list of point clouds, with optional voxel downsampling.\n    \"\"\"\n    merged = o3d.geometry.PointCloud()\n    for pcd in clouds:\n        merged += pcd\n    if voxel_size:\n        merged = merged.voxel_down_sample(voxel_size)\n    return merged\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.pose_to_transform","title":"<code>pose_to_transform(pose, *, angles_in_deg=True)</code>  <code>staticmethod</code>","text":"<p>Convert [x, y, z, rx, ry, rz] pose to a 4x4 homogeneous transformation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pose</code> <code>list[float] | ndarray</code> <p>6-DOF pose as [x, y, z, rx, ry, rz]. x, y, z in millimeters. rx, ry, rz are Euler angles (degrees or radians).</p> required <code>angles_in_deg</code> <code>bool</code> <p>Whether the input angles are in degrees (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>4x4 SE(3) transformation matrix (numpy.ndarray)</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef pose_to_transform(\n    pose: list[float] | np.ndarray,\n    *,\n    angles_in_deg: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert [x, y, z, rx, ry, rz] pose to a 4x4 homogeneous transformation matrix.\n\n    Args:\n        pose: 6-DOF pose as [x, y, z, rx, ry, rz].\n            x, y, z in millimeters.\n            rx, ry, rz are Euler angles (degrees or radians).\n        angles_in_deg: Whether the input angles are in degrees (default: True).\n\n    Returns:\n        4x4 SE(3) transformation matrix (numpy.ndarray)\n    \"\"\"\n    pose = np.asarray(pose, dtype=np.float64)\n    if pose.shape != (6,):\n        raise ValueError(\n            \"Pose must be a sequence of 6 floats: [x, y, z, rx, ry, rz]\"\n        )\n\n    position = pose[:3] / 1000.0  # mm \u2192 m\n    rotation = euler_to_matrix(*pose[3:], degrees=angles_in_deg)  # 3x3\n\n    return make_transform(rotation, position)\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.save_ply","title":"<code>save_ply(filename, points, colors=None)</code>  <code>staticmethod</code>","text":"<p>Save a point cloud to a PLY file using Open3D.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output path.</p> required <code>points</code> <code>ndarray</code> <p>(N, 3) array.</p> required <code>colors</code> <code>ndarray | None</code> <p>(N, 3) array in [0, 1] or None.</p> <code>None</code> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef save_ply(\n    filename: str,\n    points: np.ndarray,\n    colors: np.ndarray | None = None,\n) -&gt; None:\n    \"\"\"\n    Save a point cloud to a PLY file using Open3D.\n\n    Args:\n        filename: Output path.\n        points: (N, 3) array.\n        colors: (N, 3) array in [0, 1] or None.\n    \"\"\"\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    if colors is not None:\n        pcd.colors = o3d.utility.Vector3dVector(colors)\n    o3d.io.write_point_cloud(filename, pcd)\n</code></pre>"},{"location":"api/#vision.PointCloudGenerator.visualize","title":"<code>visualize(pcd)</code>  <code>staticmethod</code>","text":"<p>Visualize a point cloud using Open3D's viewer.</p> Source code in <code>vision/pointcloud/generator.py</code> <pre><code>@staticmethod\ndef visualize(pcd: o3d.geometry.PointCloud) -&gt; None:\n    \"\"\"\n    Visualize a point cloud using Open3D's viewer.\n    \"\"\"\n    o3d.visualization.draw_geometries([pcd])\n</code></pre>"},{"location":"api/#vision.RGBDAggregator","title":"<code>RGBDAggregator</code>","text":"Source code in <code>vision/pointcloud/aggregator.py</code> <pre><code>class RGBDAggregator:\n    def __init__(self, logger: LoggerType | None = None) -&gt; None:\n        self.logger = logger or Logger.get_logger(\"cloud.aggregator\")\n        self.cloud_gen = PointCloudGenerator()\n        self.transformer = TransformUtils()\n\n    def aggregate(\n        self,\n        img_pairs: list[tuple[str, str]],\n        rotations: list[np.ndarray],\n        translations: list[np.ndarray],\n        intrinsics: np.ndarray,\n        R_handeye: np.ndarray,\n        t_handeye: np.ndarray,\n        R_depth2rgb: np.ndarray,\n        t_depth2rgb: np.ndarray,\n        align: str = \"none\",\n    ) -&gt; tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"Merge frames into a single cloud with optional alignment.\"\"\"\n        base_pcd: o3d.geometry.PointCloud | None = None\n        for idx, ((rgb_path, depth_path), (R_tcp, t_tcp)) in enumerate(\n            zip(img_pairs, zip(rotations, translations))\n        ):\n            self.logger.info(f\"Processing frame {idx}: {rgb_path}, {depth_path}\")\n            rgb = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n            depth = load_depth(depth_path)\n            cam_intr = {\n                \"fx\": intrinsics[0, 0],\n                \"fy\": intrinsics[1, 1],\n                \"ppx\": intrinsics[0, 2],\n                \"ppy\": intrinsics[1, 2],\n            }\n            points, colors = self.cloud_gen.depth_to_cloud(depth, cam_intr, rgb)\n            T_rgb_ir = self.transformer.build_transform(R_depth2rgb, t_depth2rgb)\n            T_tcp_cam = self.transformer.build_transform(R_handeye, t_handeye)\n            T_tcp_rgb = T_tcp_cam @ T_rgb_ir\n            self.logger.info(f\"Frame {idx}: {points.shape[0]} points (in RGB frame).\")\n            T_base_tcp = self.transformer.build_transform(R_tcp, t_tcp)\n            T_base_cam = T_base_tcp @ T_tcp_rgb\n            points_world = self.transformer.transform_points(points, T_base_cam)\n            pcd = o3d.geometry.PointCloud()\n            pcd.points = o3d.utility.Vector3dVector(points_world)\n            if colors is not None:\n                pcd.colors = o3d.utility.Vector3dVector(colors)\n            if idx == 0:\n                base_pcd = pcd\n                self.logger.info(\"First cloud set as base.\")\n            else:\n                if align == \"open3d\":\n                    threshold = 0.003\n                    reg = o3d.pipelines.registration.registration_icp(\n                        pcd,\n                        base_pcd,\n                        threshold,\n                        np.eye(4),\n                        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n                    )\n                    rmse = reg.inlier_rmse\n                    cnt = len(reg.correspondence_set)\n                    self.logger.info(f\"ICP frame {idx}: RMSE={rmse:.5f}, pairs={cnt}\")\n                    pcd.transform(reg.transformation)\n                base_pcd += pcd\n                base_pcd = base_pcd.voxel_down_sample(voxel_size=0.003)\n        assert base_pcd is not None\n        all_points = np.asarray(base_pcd.points)\n        all_colors = np.asarray(base_pcd.colors) if base_pcd.has_colors() else None\n        return all_points, all_colors\n\n    def save_cloud(\n        self, points: np.ndarray, colors: np.ndarray | None, out_path: str\n    ) -&gt; None:\n        self.cloud_gen.save_ply(out_path, points, colors)\n        self.logger.info(f\"Aggregated cloud saved: {out_path}\")\n</code></pre>"},{"location":"api/#vision.RGBDAggregator.aggregate","title":"<code>aggregate(img_pairs, rotations, translations, intrinsics, R_handeye, t_handeye, R_depth2rgb, t_depth2rgb, align='none')</code>","text":"<p>Merge frames into a single cloud with optional alignment.</p> Source code in <code>vision/pointcloud/aggregator.py</code> <pre><code>def aggregate(\n    self,\n    img_pairs: list[tuple[str, str]],\n    rotations: list[np.ndarray],\n    translations: list[np.ndarray],\n    intrinsics: np.ndarray,\n    R_handeye: np.ndarray,\n    t_handeye: np.ndarray,\n    R_depth2rgb: np.ndarray,\n    t_depth2rgb: np.ndarray,\n    align: str = \"none\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Merge frames into a single cloud with optional alignment.\"\"\"\n    base_pcd: o3d.geometry.PointCloud | None = None\n    for idx, ((rgb_path, depth_path), (R_tcp, t_tcp)) in enumerate(\n        zip(img_pairs, zip(rotations, translations))\n    ):\n        self.logger.info(f\"Processing frame {idx}: {rgb_path}, {depth_path}\")\n        rgb = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n        depth = load_depth(depth_path)\n        cam_intr = {\n            \"fx\": intrinsics[0, 0],\n            \"fy\": intrinsics[1, 1],\n            \"ppx\": intrinsics[0, 2],\n            \"ppy\": intrinsics[1, 2],\n        }\n        points, colors = self.cloud_gen.depth_to_cloud(depth, cam_intr, rgb)\n        T_rgb_ir = self.transformer.build_transform(R_depth2rgb, t_depth2rgb)\n        T_tcp_cam = self.transformer.build_transform(R_handeye, t_handeye)\n        T_tcp_rgb = T_tcp_cam @ T_rgb_ir\n        self.logger.info(f\"Frame {idx}: {points.shape[0]} points (in RGB frame).\")\n        T_base_tcp = self.transformer.build_transform(R_tcp, t_tcp)\n        T_base_cam = T_base_tcp @ T_tcp_rgb\n        points_world = self.transformer.transform_points(points, T_base_cam)\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(points_world)\n        if colors is not None:\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n        if idx == 0:\n            base_pcd = pcd\n            self.logger.info(\"First cloud set as base.\")\n        else:\n            if align == \"open3d\":\n                threshold = 0.003\n                reg = o3d.pipelines.registration.registration_icp(\n                    pcd,\n                    base_pcd,\n                    threshold,\n                    np.eye(4),\n                    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n                )\n                rmse = reg.inlier_rmse\n                cnt = len(reg.correspondence_set)\n                self.logger.info(f\"ICP frame {idx}: RMSE={rmse:.5f}, pairs={cnt}\")\n                pcd.transform(reg.transformation)\n            base_pcd += pcd\n            base_pcd = base_pcd.voxel_down_sample(voxel_size=0.003)\n    assert base_pcd is not None\n    all_points = np.asarray(base_pcd.points)\n    all_colors = np.asarray(base_pcd.colors) if base_pcd.has_colors() else None\n    return all_points, all_colors\n</code></pre>"},{"location":"api/#vision.RealSenseD415","title":"<code>RealSenseD415</code>","text":"<p>               Bases: <code>CameraBase</code></p> <p>RealSense D415 RGB-D camera driver.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>class RealSenseD415(CameraBase):\n    \"\"\"RealSense D415 RGB-D camera driver.\"\"\"\n\n    def __init__(\n        self,\n        stream_cfg: D415_Cfg | None = None,\n        settings: D415CameraSettings | None = None,\n        filters: D415FilterConfig | None = None,\n        logger: LoggerType | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new camera instance.\n\n        Parameters\n        ----------\n        stream_cfg:\n            Optional streaming configuration.  If ``None`` the global\n            :data:`utils.settings.camera` is used.\n        settings:\n            Manual exposure and laser options.\n        filters:\n            Post-processing filter configuration.\n        logger:\n            Logger to use for all messages.\n        \"\"\"\n\n        self.stream_cfg = stream_cfg or camera\n        self.settings = settings or D415CameraSettings()\n        self.filters = filters or D415FilterConfig()\n        self.logger = logger or Logger.get_logger(\"vision.d415\")\n        self.pipeline = rs.pipeline()\n        self.config = rs.config()\n        self.started = False\n        self.profile: rs.pipeline_profile | None = None\n        self.align: rs.align | None = None\n        self.depth_scale: float = 1.0\n        self._init_config()\n\n    def _init_config(self) -&gt; None:\n        \"\"\"Pre-configure the pipeline with the desired streams.\"\"\"\n\n        cfg = self.stream_cfg\n        self.config.enable_stream(\n            rs.stream.depth,\n            cfg.depth_width,\n            cfg.depth_height,\n            rs.format.z16,\n            cfg.fps,\n        )\n        self.config.enable_stream(\n            rs.stream.color,\n            cfg.rgb_width,\n            cfg.rgb_height,\n            rs.format.bgr8,\n            cfg.fps,\n        )\n\n    def start(self) -&gt; None:\n        \"\"\"Start streaming from the camera.\"\"\"\n\n        config = rs.config()\n        config.enable_stream(\n            rs.stream.depth,\n            self.stream_cfg.depth_width,\n            self.stream_cfg.depth_height,\n            rs.format.z16,\n            self.stream_cfg.fps,\n        )\n        config.enable_stream(\n            rs.stream.color,\n            self.stream_cfg.rgb_width,\n            self.stream_cfg.rgb_height,\n            rs.format.bgr8,\n            self.stream_cfg.fps,\n        )\n        self.profile = self.pipeline.start(config)\n        device = self.profile.get_device()\n        sensors = {s.get_info(rs.camera_info.name): s for s in device.sensors}\n        sensor = sensors.get(\"Stereo Module\")\n        if sensor is None:\n            raise RuntimeError(\"Stereo Module (depth sensor) not found\")\n\n        if not sensor.supports(rs.option.depth_units):\n            raise RuntimeError(\n                \"Sensor does not support depth_units (not a depth sensor?)\"\n            )\n\n        self.depth_sensor = sensor.as_depth_sensor()\n        self.rgb_sensor = sensors.get(\"RGB Camera\")\n        if self.depth_sensor is None or self.rgb_sensor is None:\n            raise RuntimeError(\"Required sensors not found\")\n        self._apply_settings()\n        self.depth_scale = float(self.depth_sensor.get_depth_scale())\n        self.logger.info(f\"Depth scale: {self.depth_scale:.6f} m/unit\")\n        if self.stream_cfg.align_to_color:\n            self.align = rs.align(rs.stream.color)\n            self.logger.info(\"Depth frames will be aligned to color\")\n        else:\n            self.align = None\n            self.logger.info(\"No alignment between depth and color\")\n\n        self._log_device_info(device)\n        self._setup_filters()\n        self.started = True\n\n    def _apply_settings(self) -&gt; None:\n        s = self.settings\n        self.depth_sensor.set_option(rs.option.enable_auto_exposure, 1)\n        power = float(s.projector_power)\n        self.depth_sensor.set_option(rs.option.laser_power, power)\n        self.rgb_sensor.set_option(rs.option.enable_auto_exposure, 0)\n        self.rgb_sensor.set_option(rs.option.exposure, float(s.rgb_exposure))\n        self.rgb_sensor.set_option(rs.option.gain, float(s.rgb_gain))\n\n    def _setup_filters(self) -&gt; None:\n        \"\"\"Initialize depth post-processing filters.\"\"\"\n\n        cfg = self.filters\n        self.decimation = rs.decimation_filter()\n        self.decimation.set_option(rs.option.filter_magnitude, cfg.decimation)\n\n        self.spatial = rs.spatial_filter()\n        self.spatial.set_option(rs.option.filter_smooth_alpha, cfg.spatial_alpha)\n        self.spatial.set_option(rs.option.filter_smooth_delta, cfg.spatial_delta)\n\n        self.temporal = rs.temporal_filter()\n        self.temporal.set_option(rs.option.filter_smooth_alpha, cfg.temporal_alpha)\n        self.temporal.set_option(rs.option.filter_smooth_delta, cfg.temporal_delta)\n\n        self.hole_filling = rs.hole_filling_filter(cfg.hole_filling)\n\n    def _log_device_info(self, device: rs.device) -&gt; None:\n        name = device.get_info(rs.camera_info.name)\n        serial = device.get_info(rs.camera_info.serial_number)\n        self.logger.info(f\"Device: {name} SN:{serial}\")\n        depth_stream = self.profile.get_stream(\n            rs.stream.depth\n        ).as_video_stream_profile()\n        color_stream = self.profile.get_stream(\n            rs.stream.color\n        ).as_video_stream_profile()\n        extr = depth_stream.get_extrinsics_to(color_stream)\n        R = np.array(extr.rotation).reshape(3, 3)\n        t = np.array(extr.translation)\n        self.logger.debug(f\"Extrinsics depth\u2192color R={R.tolist()} t={t.tolist()}\")\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop camera streaming.\"\"\"\n\n        if self.started:\n            self.pipeline.stop()\n            self.started = False\n\n    def set_projector(self, enable: bool) -&gt; None:\n        \"\"\"Enable or disable the infrared projector.\"\"\"\n\n        power = self.settings.max_projector_power if enable else 0\n        self.depth_sensor.set_option(rs.option.laser_power, float(power))\n\n    def _process_depth(self, frame: rs.frame) -&gt; rs.frame:\n        \"\"\"Apply post-processing filters to a depth frame.\"\"\"\n        frame = self.decimation.process(frame)\n        frame = self.spatial.process(frame)\n        frame = self.temporal.process(frame)\n        frame = self.hole_filling.process(frame)\n        return frame\n\n    def get_frames(\n        self, aligned: bool = True, as_float: bool = True\n    ) -&gt; Tuple[np.ndarray | None, np.ndarray | None]:\n        \"\"\"Return the next color and depth frame pair.\n\n        Parameters\n        ----------\n        aligned : bool\n            If True, depth will be aligned to color.\n        as_float : bool\n            If True, depth will be returned in meters using depth_scale.\n\n        Returns\n        -------\n        color_img : np.ndarray or None\n            The RGB image (BGR format).\n        depth_img : np.ndarray or None\n            The depth map (aligned if specified). In meters if as_float=True.\n        \"\"\"\n        assert self.started, \"Camera not started\"\n        if aligned and not self.stream_cfg.align_to_color:\n            raise RuntimeError(\n                \"get_frames(aligned=True) is invalid because align_to_color=False\"\n            )\n\n        frames = self.pipeline.wait_for_frames()\n        if aligned and self.align:\n            frames = self.align.process(frames)\n\n        depth_frame = frames.get_depth_frame()\n        color_frame = frames.get_color_frame()\n\n        if not depth_frame or not color_frame:\n            return None, None\n\n        depth_frame = self._process_depth(depth_frame)\n        color_img = np.asanyarray(color_frame.get_data())\n        depth_img = np.asanyarray(depth_frame.get_data())\n\n        # Convert depth to float (meters)\n        if as_float:\n            depth_img = depth_img.astype(np.float32) * self.depth_scale\n\n        if aligned and color_img.shape[:2] != depth_img.shape[:2]:\n            raise RuntimeError(\"Aligned depth and color resolution mismatch\")\n\n        return color_img, depth_img\n</code></pre>"},{"location":"api/#vision.RealSenseD415.__init__","title":"<code>__init__(stream_cfg=None, settings=None, filters=None, logger=None)</code>","text":"<p>Create a new camera instance.</p>"},{"location":"api/#vision.RealSenseD415.__init__--parameters","title":"Parameters","text":"<p>stream_cfg:     Optional streaming configuration.  If <code>None</code> the global     :data:<code>utils.settings.camera</code> is used. settings:     Manual exposure and laser options. filters:     Post-processing filter configuration. logger:     Logger to use for all messages.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>def __init__(\n    self,\n    stream_cfg: D415_Cfg | None = None,\n    settings: D415CameraSettings | None = None,\n    filters: D415FilterConfig | None = None,\n    logger: LoggerType | None = None,\n) -&gt; None:\n    \"\"\"Create a new camera instance.\n\n    Parameters\n    ----------\n    stream_cfg:\n        Optional streaming configuration.  If ``None`` the global\n        :data:`utils.settings.camera` is used.\n    settings:\n        Manual exposure and laser options.\n    filters:\n        Post-processing filter configuration.\n    logger:\n        Logger to use for all messages.\n    \"\"\"\n\n    self.stream_cfg = stream_cfg or camera\n    self.settings = settings or D415CameraSettings()\n    self.filters = filters or D415FilterConfig()\n    self.logger = logger or Logger.get_logger(\"vision.d415\")\n    self.pipeline = rs.pipeline()\n    self.config = rs.config()\n    self.started = False\n    self.profile: rs.pipeline_profile | None = None\n    self.align: rs.align | None = None\n    self.depth_scale: float = 1.0\n    self._init_config()\n</code></pre>"},{"location":"api/#vision.RealSenseD415.get_frames","title":"<code>get_frames(aligned=True, as_float=True)</code>","text":"<p>Return the next color and depth frame pair.</p>"},{"location":"api/#vision.RealSenseD415.get_frames--parameters","title":"Parameters","text":"<p>aligned : bool     If True, depth will be aligned to color. as_float : bool     If True, depth will be returned in meters using depth_scale.</p>"},{"location":"api/#vision.RealSenseD415.get_frames--returns","title":"Returns","text":"<p>color_img : np.ndarray or None     The RGB image (BGR format). depth_img : np.ndarray or None     The depth map (aligned if specified). In meters if as_float=True.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>def get_frames(\n    self, aligned: bool = True, as_float: bool = True\n) -&gt; Tuple[np.ndarray | None, np.ndarray | None]:\n    \"\"\"Return the next color and depth frame pair.\n\n    Parameters\n    ----------\n    aligned : bool\n        If True, depth will be aligned to color.\n    as_float : bool\n        If True, depth will be returned in meters using depth_scale.\n\n    Returns\n    -------\n    color_img : np.ndarray or None\n        The RGB image (BGR format).\n    depth_img : np.ndarray or None\n        The depth map (aligned if specified). In meters if as_float=True.\n    \"\"\"\n    assert self.started, \"Camera not started\"\n    if aligned and not self.stream_cfg.align_to_color:\n        raise RuntimeError(\n            \"get_frames(aligned=True) is invalid because align_to_color=False\"\n        )\n\n    frames = self.pipeline.wait_for_frames()\n    if aligned and self.align:\n        frames = self.align.process(frames)\n\n    depth_frame = frames.get_depth_frame()\n    color_frame = frames.get_color_frame()\n\n    if not depth_frame or not color_frame:\n        return None, None\n\n    depth_frame = self._process_depth(depth_frame)\n    color_img = np.asanyarray(color_frame.get_data())\n    depth_img = np.asanyarray(depth_frame.get_data())\n\n    # Convert depth to float (meters)\n    if as_float:\n        depth_img = depth_img.astype(np.float32) * self.depth_scale\n\n    if aligned and color_img.shape[:2] != depth_img.shape[:2]:\n        raise RuntimeError(\"Aligned depth and color resolution mismatch\")\n\n    return color_img, depth_img\n</code></pre>"},{"location":"api/#vision.RealSenseD415.set_projector","title":"<code>set_projector(enable)</code>","text":"<p>Enable or disable the infrared projector.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>def set_projector(self, enable: bool) -&gt; None:\n    \"\"\"Enable or disable the infrared projector.\"\"\"\n\n    power = self.settings.max_projector_power if enable else 0\n    self.depth_sensor.set_option(rs.option.laser_power, float(power))\n</code></pre>"},{"location":"api/#vision.RealSenseD415.start","title":"<code>start()</code>","text":"<p>Start streaming from the camera.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start streaming from the camera.\"\"\"\n\n    config = rs.config()\n    config.enable_stream(\n        rs.stream.depth,\n        self.stream_cfg.depth_width,\n        self.stream_cfg.depth_height,\n        rs.format.z16,\n        self.stream_cfg.fps,\n    )\n    config.enable_stream(\n        rs.stream.color,\n        self.stream_cfg.rgb_width,\n        self.stream_cfg.rgb_height,\n        rs.format.bgr8,\n        self.stream_cfg.fps,\n    )\n    self.profile = self.pipeline.start(config)\n    device = self.profile.get_device()\n    sensors = {s.get_info(rs.camera_info.name): s for s in device.sensors}\n    sensor = sensors.get(\"Stereo Module\")\n    if sensor is None:\n        raise RuntimeError(\"Stereo Module (depth sensor) not found\")\n\n    if not sensor.supports(rs.option.depth_units):\n        raise RuntimeError(\n            \"Sensor does not support depth_units (not a depth sensor?)\"\n        )\n\n    self.depth_sensor = sensor.as_depth_sensor()\n    self.rgb_sensor = sensors.get(\"RGB Camera\")\n    if self.depth_sensor is None or self.rgb_sensor is None:\n        raise RuntimeError(\"Required sensors not found\")\n    self._apply_settings()\n    self.depth_scale = float(self.depth_sensor.get_depth_scale())\n    self.logger.info(f\"Depth scale: {self.depth_scale:.6f} m/unit\")\n    if self.stream_cfg.align_to_color:\n        self.align = rs.align(rs.stream.color)\n        self.logger.info(\"Depth frames will be aligned to color\")\n    else:\n        self.align = None\n        self.logger.info(\"No alignment between depth and color\")\n\n    self._log_device_info(device)\n    self._setup_filters()\n    self.started = True\n</code></pre>"},{"location":"api/#vision.RealSenseD415.stop","title":"<code>stop()</code>","text":"<p>Stop camera streaming.</p> Source code in <code>vision/camera/realsense_d415.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop camera streaming.\"\"\"\n\n    if self.started:\n        self.pipeline.stop()\n        self.started = False\n</code></pre>"},{"location":"api/#utils.D415_Cfg","title":"<code>D415_Cfg</code>  <code>dataclass</code>","text":"<p>Intel RealSense D415 camera configuration: - frame size (color/depth) - frame rate - depth scale - alignment mode</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass D415_Cfg:\n    \"\"\"\n    Intel RealSense D415 camera configuration:\n    - frame size (color/depth)\n    - frame rate\n    - depth scale\n    - alignment mode\n    \"\"\"\n\n    rgb_width: int = 1920\n    rgb_height: int = 1080\n    # rgb_width: int = 1280\n    # rgb_height: int = 720\n    depth_width: int = 1280\n    depth_height: int = 720\n    fps: int = 30\n    # TODO clarify depth_scale import at all python pakages\n    # and del DEPTH_SCALE anywhere\n    depth_scale: float = DEPTH_SCALE\n    align_to_color: bool = True\n</code></pre>"},{"location":"api/#utils.GridCalibCfg","title":"<code>GridCalibCfg</code>  <code>dataclass</code>","text":"<p>Grid-based workspace sampling for hand-eye calibration. Defines the limits, grid step, orientation, and output.</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass GridCalibCfg:\n    \"\"\"\n    Grid-based workspace sampling for hand-eye calibration.\n    Defines the limits, grid step, orientation, and output.\n    \"\"\"\n\n    calibration_type: str = \"EYE_IN_HAND\"\n    workspace_limits: tuple[\n        tuple[float, float], tuple[float, float], tuple[float, float]\n    ] = (\n        (-70.0, 50.0),  # X, mm\n        (-250.0, -130.0),  # Y, mm\n        (300.0, 400.0),  # Z, mm\n    )\n    grid_step: float = 40.0\n    tool_orientation: tuple[float, float, float] = (180.0, 0.0, 180.0)\n    charuco_xml: str = str(paths.CAMERA_INTR / \"charuco_cam.xml\")\n    calib_output_dir: str = str(paths.RESULTS_DIR)\n</code></pre>"},{"location":"api/#utils.HandEyeCfg","title":"<code>HandEyeCfg</code>  <code>dataclass</code>","text":"<p>Hand-eye calibration configuration parameters. Includes Charuco board parameters, allowed outliers, min corners, input/output paths, etc.</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass HandEyeCfg:\n    \"\"\"\n    Hand-eye calibration configuration parameters.\n    Includes Charuco board parameters, allowed outliers,\n    min corners, input/output paths, etc.\n    \"\"\"\n\n    square_numbers: tuple[int, int] = (9, 6)\n    square_length: float = 0.031\n    marker_length: float = 0.023\n    CHARUCO_DICT_MAP = {\n        \"5X5_50\": cv2.aruco.DICT_5X5_50,\n        \"5X5_100\": cv2.aruco.DICT_5X5_100,\n    }\n    aruco_dict: str = \"5X5_100\"\n    min_corners: int = 4\n    outlier_std: float = 2.0\n    method: str = \"ALL\"\n    analyze_corners: bool = False\n    visualize: bool = False\n    robot_poses_file = paths.CAPTURES_EXTR_DIR.glob(\"*.json\")  # 'calib/*.json'\n    images_dir: str = str(paths.CAPTURES_DIR)\n    charuco_xml: str = str(paths.CAMERA_INTR / \"charuco_cam.xml\")\n    charuco_txt: str = str(paths.CAMERA_INTR / \"charuco_cam.txt\")\n    calib_output_dir: str = str(paths.RESULTS_DIR)\n</code></pre>"},{"location":"api/#utils.JSONPoseLoader","title":"<code>JSONPoseLoader</code>","text":"<p>Load robot poses for hand-eye calibration from a JSON file.</p> Source code in <code>utils/io.py</code> <pre><code>class JSONPoseLoader:\n    \"\"\"Load robot poses for hand-eye calibration from a JSON file.\"\"\"\n\n    @staticmethod\n    def load_poses(json_file: str) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Return rotation and translation lists from ``json_file``.\"\"\"\n        data = load_json(json_file)\n        Rs, ts = [], []\n        for pose in data.values():\n            tcp_pose = pose[\"tcp_coords\"]\n            t = np.array(tcp_pose[:3], dtype=np.float64) / 1000.0\n            rx, ry, rz = tcp_pose[3:]\n            R_mat = Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=True).as_matrix()\n            Rs.append(R_mat)\n            ts.append(t)\n        return Rs, ts\n</code></pre>"},{"location":"api/#utils.JSONPoseLoader.load_poses","title":"<code>load_poses(json_file)</code>  <code>staticmethod</code>","text":"<p>Return rotation and translation lists from <code>json_file</code>.</p> Source code in <code>utils/io.py</code> <pre><code>@staticmethod\ndef load_poses(json_file: str) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Return rotation and translation lists from ``json_file``.\"\"\"\n    data = load_json(json_file)\n    Rs, ts = [], []\n    for pose in data.values():\n        tcp_pose = pose[\"tcp_coords\"]\n        t = np.array(tcp_pose[:3], dtype=np.float64) / 1000.0\n        rx, ry, rz = tcp_pose[3:]\n        R_mat = Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=True).as_matrix()\n        Rs.append(R_mat)\n        ts.append(t)\n    return Rs, ts\n</code></pre>"},{"location":"api/#utils.Logger","title":"<code>Logger</code>","text":"<p>Project-wide logger wrapper using loguru and global config.</p> Source code in <code>utils/logger.py</code> <pre><code>class Logger:\n    \"\"\"Project-wide logger wrapper using loguru and global config.\"\"\"\n\n    @staticmethod\n    def _configure(level: str, json_format: bool) -&gt; None:\n        \"\"\"Configure log sinks on first use.\"\"\"\n        global _is_configured, _log_file\n        _logger.remove()\n        os.makedirs(_log_dir, exist_ok=True)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        _log_file = _log_dir / f\"{timestamp}.log.json\"\n        _logger.add(\n            sys.stdout,\n            level=level,\n            serialize=False,\n            format=LOGCFG.log_format,\n        )\n        _logger.add(\n            _log_file,\n            level=level,\n            serialize=json_format,\n            format=LOGCFG.log_file_format,\n        )\n        _is_configured = True\n\n    @staticmethod\n    def get_logger(\n        name: str, level: str = None, json_format: bool = None\n    ) -&gt; LoguruLogger:\n        \"\"\"\n        Return a configured loguru logger bound to ``name``.\n        If level or json_format are not specified, uses global config.\n        \"\"\"\n        global _is_configured\n        if not _is_configured:\n            Logger._configure(\n                level or LOGCFG.level,\n                json_format if json_format is not None else LOGCFG.json,\n            )\n        return _logger.bind(module=name)\n\n    @staticmethod\n    def progress(\n        iterable: Iterable[T],\n        desc: str | None = None,\n        total: int | None = None,\n    ) -&gt; Iterable[T]:\n        \"\"\"Return a tqdm iterator with unified style.\"\"\"\n        return cast(\n            Iterable[T],\n            tqdm(\n                iterable,\n                desc=desc,\n                total=total,\n                leave=False,\n                bar_format=LOGCFG.progress_bar_format,\n            ),\n        )\n\n    @staticmethod\n    def configure_root_logger(level: str = \"WARNING\") -&gt; None:\n        \"\"\"Configure the root logger for third-party libraries.\"\"\"\n        global _is_configured\n        _logger.remove()\n        _logger.add(sys.stdout, level=level)\n        _is_configured = True\n\n    @staticmethod\n    def configure(\n        level: str = None, log_dir: str | Path = None, json_format: bool = None\n    ) -&gt; None:\n        \"\"\"Manually configure the logger with given settings.\"\"\"\n        global _log_dir\n        _log_dir = Path(log_dir) if log_dir is not None else LOGCFG.log_dir\n        Logger._configure(\n            level or LOGCFG.level,\n            json_format if json_format is not None else LOGCFG.json,\n        )\n</code></pre>"},{"location":"api/#utils.Logger.configure","title":"<code>configure(level=None, log_dir=None, json_format=None)</code>  <code>staticmethod</code>","text":"<p>Manually configure the logger with given settings.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef configure(\n    level: str = None, log_dir: str | Path = None, json_format: bool = None\n) -&gt; None:\n    \"\"\"Manually configure the logger with given settings.\"\"\"\n    global _log_dir\n    _log_dir = Path(log_dir) if log_dir is not None else LOGCFG.log_dir\n    Logger._configure(\n        level or LOGCFG.level,\n        json_format if json_format is not None else LOGCFG.json,\n    )\n</code></pre>"},{"location":"api/#utils.Logger.configure_root_logger","title":"<code>configure_root_logger(level='WARNING')</code>  <code>staticmethod</code>","text":"<p>Configure the root logger for third-party libraries.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef configure_root_logger(level: str = \"WARNING\") -&gt; None:\n    \"\"\"Configure the root logger for third-party libraries.\"\"\"\n    global _is_configured\n    _logger.remove()\n    _logger.add(sys.stdout, level=level)\n    _is_configured = True\n</code></pre>"},{"location":"api/#utils.Logger.get_logger","title":"<code>get_logger(name, level=None, json_format=None)</code>  <code>staticmethod</code>","text":"<p>Return a configured loguru logger bound to <code>name</code>. If level or json_format are not specified, uses global config.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef get_logger(\n    name: str, level: str = None, json_format: bool = None\n) -&gt; LoguruLogger:\n    \"\"\"\n    Return a configured loguru logger bound to ``name``.\n    If level or json_format are not specified, uses global config.\n    \"\"\"\n    global _is_configured\n    if not _is_configured:\n        Logger._configure(\n            level or LOGCFG.level,\n            json_format if json_format is not None else LOGCFG.json,\n        )\n    return _logger.bind(module=name)\n</code></pre>"},{"location":"api/#utils.Logger.progress","title":"<code>progress(iterable, desc=None, total=None)</code>  <code>staticmethod</code>","text":"<p>Return a tqdm iterator with unified style.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef progress(\n    iterable: Iterable[T],\n    desc: str | None = None,\n    total: int | None = None,\n) -&gt; Iterable[T]:\n    \"\"\"Return a tqdm iterator with unified style.\"\"\"\n    return cast(\n        Iterable[T],\n        tqdm(\n            iterable,\n            desc=desc,\n            total=total,\n            leave=False,\n            bar_format=LOGCFG.progress_bar_format,\n        ),\n    )\n</code></pre>"},{"location":"api/#utils.TransformUtils","title":"<code>TransformUtils</code>  <code>dataclass</code>","text":"<p>Utility class for chaining and applying transformations.</p> Source code in <code>utils/geometry.py</code> <pre><code>@dataclass\nclass TransformUtils:\n    \"\"\"Utility class for chaining and applying transformations.\"\"\"\n\n    logger: Logger | None = None\n\n    def __post_init__(self) -&gt; None:  # noqa: D401\n        self.logger = self.logger or Logger.get_logger(\"utils.transform\")\n\n    @staticmethod\n    def build_transform(R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n        return make_transform(R, t)\n\n    @staticmethod\n    def apply_transform(points: np.ndarray, R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n        T = make_transform(R, t)\n        points_h = np.hstack([points, np.ones((points.shape[0], 1))])\n        return (T @ points_h.T).T[:, :3]\n\n    @staticmethod\n    def chain_transforms(*Ts: np.ndarray) -&gt; np.ndarray:\n        T_out = np.eye(4)\n        for T in Ts:\n            T_out = T_out @ T\n        return T_out\n\n    def transform_points(self, points: np.ndarray, T: np.ndarray) -&gt; np.ndarray:\n        self.logger.debug(f\"Applying transform to {points.shape[0]} points\")\n        points_h = np.hstack([points, np.ones((points.shape[0], 1))])\n        transformed = (T @ points_h.T).T[:, :3]\n        self.logger.debug(\n            f\"Transformed points sample: {transformed[:2].tolist()}\"\n        )\n        return transformed\n\n    def base_to_tcp(\n        self, tcp_pose: np.ndarray | tuple[np.ndarray, np.ndarray]\n    ) -&gt; np.ndarray:\n        if isinstance(tcp_pose, np.ndarray) and tcp_pose.shape == (4, 4):\n            return tcp_pose\n        if isinstance(tcp_pose, (tuple, list)) and len(tcp_pose) == 2:\n            R, t = tcp_pose\n            return make_transform(R, t)\n        raise ValueError(\"tcp_pose must be SE(3) 4x4 or (R, t)\")\n\n    def tool_to_tcp(self, tcp_offset: np.ndarray | None) -&gt; np.ndarray:\n        if tcp_offset is None or np.allclose(tcp_offset, 0):\n            return np.eye(4)\n        x, y, z, rx, ry, rz = tcp_offset\n        rot = euler_to_matrix(rx, ry, rz, degrees=True)\n        return make_transform(rot, np.array([x, y, z]))\n\n    def tcp_to_camera(self, R_handeye: np.ndarray, t_handeye: np.ndarray) -&gt; np.ndarray:\n        return make_transform(R_handeye, t_handeye)\n\n    def get_base_to_camera(\n        self,\n        tcp_pose: np.ndarray | tuple[np.ndarray, np.ndarray],\n        tcp_offset: np.ndarray | None,\n        R_handeye: np.ndarray,\n        t_handeye: np.ndarray,\n    ) -&gt; np.ndarray:\n        T_base_tcp = self.base_to_tcp(tcp_pose)\n        T_tcp_tool = self.tool_to_tcp(tcp_offset)\n        T_tool_cam = self.tcp_to_camera(R_handeye, t_handeye)\n        T_base_cam = self.chain_transforms(T_base_tcp, T_tcp_tool, T_tool_cam)\n        self.logger.info(\"Computed T_base\u2192camera\")\n        return T_base_cam\n\n    def camera_to_world(\n        self, points_cam: np.ndarray, T_base_cam: np.ndarray\n    ) -&gt; np.ndarray:\n        self.logger.info(\"Transforming points: camera \u2192 world\")\n        return self.transform_points(points_cam, T_base_cam)\n\n    def world_to_camera(\n        self, points_world: np.ndarray, T_base_cam: np.ndarray\n    ) -&gt; np.ndarray:\n        self.logger.info(\"Transforming points: world \u2192 camera\")\n        return self.transform_points(points_world, invert_transform(T_base_cam))\n\n    def camera_to_tcp(\n        self, points_cam: np.ndarray, R_handeye: np.ndarray, t_handeye: np.ndarray\n    ) -&gt; np.ndarray:\n        T = self.tcp_to_camera(R_handeye, t_handeye)\n        return self.transform_points(points_cam, invert_transform(T))\n\n    def tcp_to_camera_points(\n        self, points_tcp: np.ndarray, R_handeye: np.ndarray, t_handeye: np.ndarray\n    ) -&gt; np.ndarray:\n        T = self.tcp_to_camera(R_handeye, t_handeye)\n        return self.transform_points(points_tcp, T)\n</code></pre>"},{"location":"api/#utils.decompose_transform","title":"<code>decompose_transform(T)</code>","text":"<p>Return rotation matrix and translation vector from a transform.</p> Source code in <code>utils/geometry.py</code> <pre><code>def decompose_transform(T: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return rotation matrix and translation vector from a transform.\"\"\"\n    return T[:3, :3], T[:3, 3]\n</code></pre>"},{"location":"api/#utils.euler_to_matrix","title":"<code>euler_to_matrix(rx, ry, rz, *, degrees=True)</code>","text":"<p>Return a rotation matrix from Euler angles.</p> Source code in <code>utils/geometry.py</code> <pre><code>def euler_to_matrix(\n    rx: float, ry: float, rz: float, *, degrees: bool = True\n) -&gt; np.ndarray:\n    \"\"\"Return a rotation matrix from Euler angles.\"\"\"\n    return Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=degrees).as_matrix()\n</code></pre>"},{"location":"api/#utils.invert_transform","title":"<code>invert_transform(T)</code>","text":"<p>Return the inverse of a homogeneous transform.</p> Source code in <code>utils/geometry.py</code> <pre><code>def invert_transform(T: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return the inverse of a homogeneous transform.\"\"\"\n    R, t = decompose_transform(T)\n    R_inv = R.T\n    t_inv = -R_inv @ t\n    return make_transform(R_inv, t_inv)\n</code></pre>"},{"location":"api/#utils.load_camera_params","title":"<code>load_camera_params(path)</code>","text":"<p>Read camera matrix and distortion coefficients from OpenCV XML/YAML.</p> Source code in <code>utils/io.py</code> <pre><code>def load_camera_params(path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Read camera matrix and distortion coefficients from OpenCV XML/YAML.\"\"\"\n    fs = cv2.FileStorage(str(path), cv2.FILE_STORAGE_READ)\n    camera_matrix = fs.getNode(\"camera_matrix\").mat()\n    dist_coeffs = fs.getNode(\"dist_coeffs\").mat()\n    fs.release()\n    return camera_matrix, dist_coeffs\n</code></pre>"},{"location":"api/#utils.load_json","title":"<code>load_json(path)</code>","text":"<p>Load JSON data from <code>path</code>.</p> Source code in <code>utils/io.py</code> <pre><code>def load_json(path: str) -&gt; Any:\n    \"\"\"Load JSON data from ``path``.\"\"\"\n    with open(path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#utils.load_npy","title":"<code>load_npy(path)</code>","text":"<p>Load an <code>.npy</code> array.</p> Source code in <code>utils/io.py</code> <pre><code>def load_npy(path: str) -&gt; np.ndarray:\n    \"\"\"Load an ``.npy`` array.\"\"\"\n    return np.load(path)\n</code></pre>"},{"location":"api/#utils.make_transform","title":"<code>make_transform(R, t)</code>","text":"<p>Build a homogeneous transform from <code>R</code> and <code>t</code>.</p> Source code in <code>utils/geometry.py</code> <pre><code>def make_transform(R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build a homogeneous transform from ``R`` and ``t``.\"\"\"\n    T = np.eye(4)\n    T[:3, :3] = R\n    T[:3, 3] = t.flatten()\n    return T\n</code></pre>"},{"location":"api/#utils.read_image","title":"<code>read_image(path)</code>","text":"<p>Return an image from <code>path</code> or <code>None</code> if loading fails.</p> Source code in <code>utils/io.py</code> <pre><code>def read_image(path: str) -&gt; np.ndarray | None:\n    \"\"\"Return an image from ``path`` or ``None`` if loading fails.\"\"\"\n    return cv2.imread(path)\n</code></pre>"},{"location":"api/#utils.save_camera_params_txt","title":"<code>save_camera_params_txt(path, K, dist, rms=None)</code>","text":"<p>Write camera parameters to a text file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_camera_params_txt(\n    path: str, K: np.ndarray, dist: np.ndarray, rms: float | None = None\n) -&gt; None:\n    \"\"\"Write camera parameters to a text file.\"\"\"\n    with open(path, \"w\") as f:\n        if rms is not None:\n            f.write(f\"RMS Error: {rms:.6f}\\n\")\n        f.write(\"camera_matrix =\\n\")\n        np.savetxt(f, K, fmt=\"%.10f\")\n        f.write(\"dist_coeffs =\\n\")\n        np.savetxt(f, dist.reshape(1, -1), fmt=\"%.10f\")\n</code></pre>"},{"location":"api/#utils.save_camera_params_xml","title":"<code>save_camera_params_xml(path, K, dist)</code>","text":"<p>Write camera parameters to an XML/YAML file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_camera_params_xml(path: str, K: np.ndarray, dist: np.ndarray) -&gt; None:\n    \"\"\"Write camera parameters to an XML/YAML file.\"\"\"\n    fs = cv2.FileStorage(str(path), cv2.FILE_STORAGE_WRITE)\n    fs.write(\"camera_matrix\", K)\n    fs.write(\"dist_coeffs\", dist)\n    fs.release()\n</code></pre>"},{"location":"api/#utils.save_json","title":"<code>save_json(path, data)</code>","text":"<p>Write data as JSON to <code>path</code>.</p> Source code in <code>utils/io.py</code> <pre><code>def save_json(path: str, data: Any) -&gt; None:\n    \"\"\"Write data as JSON to ``path``.\"\"\"\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/#utils.save_npy","title":"<code>save_npy(path, arr)</code>","text":"<p>Save an array to an <code>.npy</code> file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_npy(path: str, arr: np.ndarray) -&gt; None:\n    \"\"\"Save an array to an ``.npy`` file.\"\"\"\n    np.save(path, arr)\n</code></pre>"},{"location":"api/#utils.write_image","title":"<code>write_image(path, img)</code>","text":"<p>Save an image to disk.</p> Source code in <code>utils/io.py</code> <pre><code>def write_image(path: str, img: np.ndarray) -&gt; None:\n    \"\"\"Save an image to disk.\"\"\"\n    cv2.imwrite(path, img)\n</code></pre>"},{"location":"handeye/","title":"HANDEYE","text":"<p>This document explains the full chain of transformations used to convert 2D pixel measurements and depth values into 3D robot base coordinates. Each step is mathematically grounded and annotated for clarity. References to OpenCV and robotics sources are included.</p>"},{"location":"handeye/#1-full-coordinate-transformation-chain","title":"1. Full Coordinate Transformation Chain","text":"\\[ \\begin{bmatrix} u \\\\ v \\\\ d \\end{bmatrix} \\xrightarrow{\\mathrm{K_{depth}}} \\mathrm{p_{depth}} \\xrightarrow{\\mathrm{T_{depth2rgb}}} \\mathrm{p_{rgb}} \\xrightarrow{\\mathrm{T_{cam2base}}} \\mathrm{p_{base}} \\] <p>Each arrow represents a transformation:</p> <ul> <li>\\(\\mathrm{K_{depth}}\\): Intrinsic matrix of the depth camera.</li> <li>\\(\\mathrm{T_{depth2rgb}}\\): Extrinsic calibration between Depth and RGB sensors.</li> <li>\\(\\mathrm{T_{cam2base}}\\): Robot pose transform from RGB sensors of camera through TCP to base.</li> </ul>"},{"location":"handeye/#11-intrinsic-camera-matrix-opencv-model","title":"1.1 Intrinsic Camera Matrix (OpenCV Model)","text":"\\[ \\mathrm{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}  \\] <ul> <li>\\(f_x, f_y\\): Focal lengths in pixels.</li> <li>\\(c_x, c_y\\): Principal point coordinates.</li> </ul> <p>Reference: OpenCV Camera Calibration</p>"},{"location":"handeye/#12-backprojection-from-rgb-image-to-3d","title":"1.2 Backprojection from RGB image to 3D","text":"\\[ \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} (u - c_x) \\cdot \\dfrac{d}{f_x} \\\\ (v - c_y) \\cdot \\dfrac{d}{f_y} \\\\ d \\end{bmatrix} \\qquad \\begin{aligned} d~&amp;-\\text{depth value (m)} \\\\ (u, v)~&amp;-\\text{RGB image coordinates (pixels)} \\\\ (x, y, z)~&amp;-\\text{camera coordinates} \\end{aligned} \\] <p>This converts depth and pixel coordinates into 3D points in the camera coordinate system.</p>"},{"location":"handeye/#2-camera-to-base-transform","title":"2. Camera-to-Base Transform","text":"\\[  \\mathrm{T_{cam2base}} = \\mathrm{T_{tcp2base}} \\times \\mathrm{T_{cam2tcp}}  \\]"},{"location":"handeye/#21-camera-to-tcp-transformation","title":"2.1 Camera-to-TCP Transformation","text":"\\[  \\mathrm{T_{cam2tcp}} = \\begin{bmatrix} R &amp; t\\\\ 0 &amp; 1  \\end{bmatrix}  \\] <p>Rigid transformation from the camera coordinate system to the robot TCP (tool center point).</p> <p>R: \\(3\\times3\\) rotation matrix.</p> <p>t: \\(3\\times1\\) translation vector.</p> <p>Reference: cv2.calibrateHandEye</p> \\[ \\mathrm{TARGET\\_POSE} = [   \\underset{mm}{x, y, z},\\    \\overset{degree}{r_x, r_y, r_z} ] \\] <p>Target Pose of in robot base coords.</p> <p>\\([x, y, z]\\): Translation of the object in millimeters.</p> <p>\\([r_x, r_y, r_z]\\): Euler angles (XYZ order) in degrees.</p>"},{"location":"handeye/#22-tcp-to-base-transformation","title":"2.2 TCP-to-Base Transformation","text":"\\[ \\mathrm{T_{tcp2base}} = \\begin{bmatrix} R(r_x, r_y, r_z) &amp; 0.001 \\cdot \\left[ x, y, z\\right]^T \\\\ 0 &amp; 1  \\end{bmatrix}  \\] <p>\\(R(r_x, r_y, r_z)\\): Rotation matrix from Euler angles (converted to radians).</p> <p>\\(0.001 \\cdot \\left[ x, y, z\\right]^T\\) : Converts mm to meters for robotics.</p>"},{"location":"handeye/#23-euler-angle-rotation-matrix","title":"2.3 Euler Angle Rotation Matrix","text":"\\[ R(r_x, r_y, r_z) = \\begin{bmatrix} c_{r_y} c_{r_z} &amp; c_{r_x} s_{r_y} s_{r_z} - s_{r_x} c_{r_z} &amp; c_{r_x} s_{r_y} c_{r_z} + s_{r_x} s_{r_z} \\\\ c_{r_y} s_{r_z} &amp; s_{r_x} s_{r_y} s_{r_z} + c_{r_x} c_{r_z} &amp; c_{r_z} s_{r_x} s_{r_y} - c_{r_x} s_{r_z} \\\\ -s_{r_y} &amp; c_{r_y} s_{r_x} &amp; c_{r_y} c_{r_x} \\end{bmatrix} \\] \\[ \\begin{aligned} c_{r_x} = \\cos(r_x),\\quad c_{r_y} = \\cos(r_y),\\quad c_{r_z} = \\cos(r_z) \\\\ s_{r_x} = \\sin(r_x),\\quad s_{r_y} = \\sin(r_y),\\quad s_{r_z} = \\sin(r_z) \\end{aligned} \\] <p>This formulation follows XYZ (roll-pitch-yaw) convention.</p>"},{"location":"handeye/#3-hand-eye-calibration","title":"3. Hand-Eye Calibration","text":"<p>OpenCV implements several closed-form methods for solving the hand\u2013eye calibration problem, which is formulated as the matrix equation:</p> \\[ \\mathrm{A}_i \\mathrm{X} = \\mathrm{X} \\mathrm{B}_i \\] <ul> <li>\\(\\mathrm{A}_i = \\mathrm{T_{robot}}^{(i)}\\): the known robot pose from base to TCP at time \\(i\\).</li> <li>\\(\\mathrm{B}_i = \\mathrm{T_{target}}^{(i)}\\): the known pose of a calibration target (e.g., Chess board, Charuco board) relative to the camera</li> <li>\\(\\mathrm{X} = \\mathrm{T_{cam2tcp}}\\): the unknown camera-to-TCP transformation to solve for</li> </ul> <p>This is a form of the Sylvester-type equation, arising in rigid body calibration problems.</p> <p>OpenCV\u2019s <code>cv2.calibrateHandEye</code> solves this problem using one of several algorithms:</p> <ul> <li>Tsai &amp; Lenz (1989) \u2014 classic decoupled rotation/translation method</li> <li>Park &amp; Martin (1994) \u2014 screw-based method</li> <li>Horaud et al. (1995) \u2014 quaternion-based optimization</li> <li>Daniilidis (1999) \u2014 dual quaternion approach</li> <li>Andreff et al. (2001) \u2014 algebraic method minimizing reprojection error</li> </ul> <p>The calibration minimizes:</p> \\[ \\sum_i \\\\| \\mathrm{A}_i \\mathrm{X} - \\mathrm{X} \\mathrm{B}_i \\\\|^2 \\]"},{"location":"handeye/#references","title":"References","text":"<ul> <li>OpenCV: cv2.calibrateHandEye \u2014 Hand-eye calibration routine</li> <li>OpenCV Camera Calibration \u2014 Intrinsic matrix and distortion model</li> <li>OpenCV Pose Estimation (solvePnP) \u2014 Camera pose from 2D\u20133D points</li> <li>Wikipedia: Euler Angles \u2014 Rotation from roll, pitch, yaw</li> <li>Wikipedia: Transformation Matrix \u2014 Homogeneous coordinate transforms</li> <li>Wikipedia: Sylvester Equation \u2014 Sylvester matrix equations</li> </ul>"},{"location":"math/","title":"Mathematical Details","text":""},{"location":"math/#perspectivenpoint","title":"Perspective\u2011n\u2011Point","text":"<p>Board poses are estimated by solving the PnP problem:</p> \\[ \\begin{aligned}  r\\_vec, t\\_vec &amp;= \\mathrm{solvePnP}(X\\_{obj}, x\\_{img}, K, d)\\\\  X\\_{cam} &amp;= R\\;X\\_{obj} + t \\end{aligned} \\] <p>where \\(X\\_{obj}\\) are known 3\u2011D points on the target and \\(x\\_{img}\\) are the detected pixel coordinates.</p>"},{"location":"math/#handeye-calibration","title":"Hand\u2011Eye Calibration","text":"<p>OpenCV implements several closed\u2011form solutions to \\(AX = XB\\) where</p> \\[ A\\_i = T\\_{robot}^{(i)}, \\quad B\\_i = T\\_{target}^{(i)}. \\] <p>The function <code>cv2.calibrateHandEye</code> returns rotation \\(R\\_{he}\\) and translation \\(t\\_{he}\\) that minimize</p> \\[ \\sum\\_i \\| A\\_i X - X B\\_i \\|^2. \\] <p>See Tsai &amp; Lenz, 1989 for the classical formulation.</p>"},{"location":"pipeline/","title":"Hand\u2011Eye Calibration Pipeline","text":"<p>This section summarizes the calibration workflow implemented in <code>handeye_chess.py</code> and <code>handeye_charuco.py</code>.</p>"},{"location":"pipeline/#overview","title":"Overview","text":"<ol> <li>Load camera parameters and robot poses. Intrinsics <code>K</code> and distortion <code>d</code> are read from <code>cam_params.yml</code>. Depth\u2011to\u2011RGB extrinsics <code>R_ext</code>, <code>t_ext</code> provide sensor alignment.</li> <li>Detect the calibration pattern. Chess boards use <code>cv2.findChessboardCorners</code>; Charuco boards use <code>cv2.aruco.detectMarkers</code> followed by <code>cv2.aruco.interpolateCornersCharuco</code>.</li> <li>Filter detections by reprojection error and board pose quality.</li> <li>Estimate the board pose via <code>solvePnP</code> to obtain rotation <code>R</code> and translation <code>t</code> for each frame.</li> <li>Assemble camera\u2194robot pairs and run <code>cv2.calibrateHandEye</code> using OpenCV\u2019s Tsai/Park/Horaud/Andreff/Daniilidis methods.</li> <li>Save results and diagnostics. The final hand\u2013eye matrix is written to <code>handeye_res.yaml</code> together with per\u2011frame logs and overlay images.</li> </ol> <pre><code>RGB/Depth frames \u2500\u2500\u25ba detect pattern \u2500\u2500\u25ba solvePnP \u2500\u2500\u25ba hand\u2011eye calibrate \u2500\u2500\u25ba YAML\n</code></pre> <p>All measurements use meters and degrees. Input images are loaded from <code>calib/imgs/frame_*.png</code> with matching depth maps <code>frame_*.npy</code>. Robot poses are stored as <code>{ \"tcp_coords\": [x, y, z, Rx, Ry, Rz] }</code>.</p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#calibration-workflow","title":"Calibration Workflow","text":"<p>Calibration utilities operate on captured RGB/D frames and recorded robot poses. A typical session:</p> <ol> <li>Capture data: <code>python -m calibration.runner_camera --count 20</code></li> <li>Move the robot on a grid: <code>python -m calibration.runner_robot</code></li> <li>Run hand\u2013eye calibration: <code>python handeye_charuco.py</code></li> </ol> <p>Results are stored under <code>calibration/results/</code>.</p>"},{"location":"usage/#robot-cli","title":"Robot CLI","text":"<p>The <code>robot</code> package exposes a small command line interface for common tasks:</p> <pre><code>python -m robot.cli record --ip &lt;robot_ip&gt; --captures_dir ./captures\n</code></pre> <p>Arguments are defined in the <code>robot.cli</code> module:</p> <pre><code>    def _add_record_args(parser: argparse.ArgumentParser) -&gt; None:\n        \"\"\"Arguments for the ``record`` subcommand.\"\"\"\n        parser.add_argument(\"--ip\", default=robot.ip, help=\"Robot IP\")\n        parser.add_argument(\n            \"--captures_dir\",\n            default=str(paths.CAPTURES_DIR),\n            help=\"Directory for saved poses\",\n        )\n        parser.add_argument(\"--drag\", action=\"store_true\", help=\"Enable drag teaching mode\")\n        parser.add_argument(\n            \"--db_path\", default=\"robot_data.lmdb\", help=\"LMDB database path\"\n        )\n        parser.add_argument(\"--use-db\", action=\"store_true\", help=\"Use LMDB storage\")\n</code></pre> <p>Use <code>robot.cli restart</code> to reconnect to the controller if the connection drops.</p>"}]}
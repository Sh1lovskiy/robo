{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Robotics Vision &amp; Calibration Suite","text":"<p>Welcome to the project documentation. The repository groups several robotics and computer vision utilities under a single umbrella. The core packages are:</p> <ul> <li><code>calibration/</code> \u2013 algorithms for intrinsic camera calibration, hand\u2011eye estimation and data collection.</li> <li><code>robot/</code> \u2013 a minimal robot control API with pose recording workflows.</li> <li><code>robot_scan/</code> \u2013 RGB\u2011D capture, point\u2011cloud processing and robot\u2011guided scanning.</li> <li><code>utils/</code> \u2013 shared helpers for logging, CLI dispatch and geometry math.</li> </ul> <p>The README describes the directory layout in detail. Below is an excerpt:</p> <pre><code>project-root/\n\u2502\n\u251c\u2500\u2500 calibration/          # Calibration algorithms &amp; workflows\n\u2502   \u251c\u2500\u2500 pattern.py              # Unified calibration patterns\n\u2502   \u251c\u2500\u2500 handeye.py              # Hand-eye calibration helpers\n\u2502   \u251c\u2500\u2500 pose_loader.py          # Load robot poses from JSON\n\u2502   \u251c\u2500\u2500 workflows.py            # High-level calibration routines &amp; CLI\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 robot/                # Robot API &amp; workflows\n\u2502   \u251c\u2500\u2500 controller.py           # Main control class\n\u2502   \u251c\u2500\u2500 workflows.py            # Pose recorder and path runner\n\u2502   \u251c\u2500\u2500 marker.py               # Simple marker utilities\n\u2502   \u251c\u2500\u2500 Robot.py                # Cython RPC bindings\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 utils/                # Common utilities and storage\n\u2502   \u251c\u2500\u2500 logger.py               # Centralized, JSON-capable logger\n\u2502   \u251c\u2500\u2500 error_tracker.py        # Global exception and signal handling\n\u2502   \u251c\u2500\u2500 cli.py                  # CommandDispatcher helper\n\u2502   \u251c\u2500\u2500 keyboard.py             # Global hotkey listener\n\u2502   \u251c\u2500\u2500 io.py                   # File and camera parameter I/O\n\u2502   \u2514\u2500\u2500 README.md               # Package overview\n\u2502\n\u251c\u2500\u2500 robot_scan/           # Robot-guided scanning pipeline\n\u2502   \u251c\u2500\u2500 capture.py             # RealSense RGB\u2011D frame capture\n\u2502   \u251c\u2500\u2500 graph.py               # Skeleton graph construction\n\u2502   \u251c\u2500\u2500 main.py                # End-to-end CLI pipeline\n\u2502   \u251c\u2500\u2500 motion.py              # Robot motion helpers\n\u2502   \u251c\u2500\u2500 preprocess.py          # Cloud filtering and plane detection\n\u2502   \u251c\u2500\u2500 save.py                # Data export utilities\n\u2502   \u251c\u2500\u2500 skeleton.py            # 2D skeletonization helpers\n\u2502   \u2514\u2500\u2500 visualization.py       # Plotly-based inspection tools\n</code></pre> <p>Use the navigation on the left to explore usage examples, pipeline descriptions and the full Python API.</p>"},{"location":"api/","title":"API Reference","text":"<p>The full Python API is generated from type\u2011hints and docstrings using mkdocstrings. Use the module links below to explore each package.</p>"},{"location":"api/#packages","title":"Packages","text":"<ul> <li><code>calibration</code></li> <li><code>robot</code></li> <li><code>robot_scan</code></li> <li><code>utils</code></li> </ul> <p>Unified camera calibration package.</p> <p>Robot scanning and motion pipeline package.</p> <p>Shared helper modules used across the project.</p> <p>The :mod:<code>utils</code> package contains lightweight helpers for logging, CLI dispatching and simple file I/O. These utilities are used by most other packages and avoid additional dependencies.</p>"},{"location":"api/#calibration.CalibrationPattern","title":"<code>CalibrationPattern</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for calibration patterns.</p> Source code in <code>calibration/base.py</code> <pre><code>class CalibrationPattern(ABC):\n    \"\"\"Abstract base class for calibration patterns.\"\"\"\n\n    def __init__(self, board_size: Tuple[int, int], square_length: float) -&gt; None:\n        self.board_size = board_size\n        self.square_length = square_length\n\n    @abstractmethod\n    def detect(self, image: np.ndarray) -&gt; Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"Detect pattern points in ``image``.\n\n        Returns\n        -------\n        tuple of ``(object_points, image_points, overlay)`` or ``None`` if the\n        pattern is not found.\n        \"\"\"\n</code></pre>"},{"location":"api/#calibration.CalibrationPattern.detect","title":"<code>detect(image)</code>  <code>abstractmethod</code>","text":"<p>Detect pattern points in <code>image</code>.</p>"},{"location":"api/#calibration.CalibrationPattern.detect--returns","title":"Returns","text":"<p>tuple of <code>(object_points, image_points, overlay)</code> or <code>None</code> if the pattern is not found.</p> Source code in <code>calibration/base.py</code> <pre><code>@abstractmethod\ndef detect(self, image: np.ndarray) -&gt; Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Detect pattern points in ``image``.\n\n    Returns\n    -------\n    tuple of ``(object_points, image_points, overlay)`` or ``None`` if the\n    pattern is not found.\n    \"\"\"\n</code></pre>"},{"location":"api/#calibration.Calibrator","title":"<code>Calibrator</code>","text":"<p>Solve camera-to-target poses using a specific pattern.</p> Source code in <code>calibration/base.py</code> <pre><code>class Calibrator:\n    \"\"\"Solve camera-to-target poses using a specific pattern.\"\"\"\n\n    def __init__(\n        self,\n        pattern: CalibrationPattern,\n        K: np.ndarray,\n        dist: np.ndarray,\n        *,\n        save_images: bool = False,\n    ) -&gt; None:\n        self.pattern = pattern\n        self.K = K\n        self.dist = dist\n        self.save_images = save_images\n\n    # ------------------------------------------------------------------\n    def run(self, image_dir: Path) -&gt; None:\n        \"\"\"Run pose estimation on images within ``image_dir``.\n\n        Parameters\n        ----------\n        image_dir:\n            Directory containing ``*_rgb.png`` and ``*_depth.npy`` pairs.\n        \"\"\"\n\n        pairs = load_image_pairs(image_dir)\n        poses: List[np.ndarray] = []\n        for idx, pair in enumerate(tqdm(pairs, desc=\"Frames\")):\n            rgb = cv2.imread(str(pair.rgb))\n            if rgb is None:\n                log.warning(\"Skipping %s: missing RGB image\", pair.rgb.name)\n                continue\n            det = self.pattern.detect(rgb)\n            if det is None:\n                log.warning(\"Pattern not detected in frame %d\", idx)\n                continue\n            obj_pts, img_pts, overlay = det\n            pose, err = estimate_pose_pnp(obj_pts, img_pts, self.K, self.dist)\n            poses.append(pose)\n            if self.save_images and overlay is not None:\n                cv2.imwrite(str(image_dir / f\"{idx:03d}_overlay.png\"), overlay)\n            log.debug(\"Frame %d reprojection error %.3f\", idx, err)\n\n        if not poses:\n            log.error(\"No valid poses computed\")\n            return\n\n        out_json = image_dir / \"camera_poses.json\"\n        save_poses(poses, out_json)\n        log.info(\"Calibration finished -&gt; %s\", out_json)\n</code></pre>"},{"location":"api/#calibration.Calibrator.run","title":"<code>run(image_dir)</code>","text":"<p>Run pose estimation on images within <code>image_dir</code>.</p>"},{"location":"api/#calibration.Calibrator.run--parameters","title":"Parameters","text":"<p>image_dir:     Directory containing <code>*_rgb.png</code> and <code>*_depth.npy</code> pairs.</p> Source code in <code>calibration/base.py</code> <pre><code>def run(self, image_dir: Path) -&gt; None:\n    \"\"\"Run pose estimation on images within ``image_dir``.\n\n    Parameters\n    ----------\n    image_dir:\n        Directory containing ``*_rgb.png`` and ``*_depth.npy`` pairs.\n    \"\"\"\n\n    pairs = load_image_pairs(image_dir)\n    poses: List[np.ndarray] = []\n    for idx, pair in enumerate(tqdm(pairs, desc=\"Frames\")):\n        rgb = cv2.imread(str(pair.rgb))\n        if rgb is None:\n            log.warning(\"Skipping %s: missing RGB image\", pair.rgb.name)\n            continue\n        det = self.pattern.detect(rgb)\n        if det is None:\n            log.warning(\"Pattern not detected in frame %d\", idx)\n            continue\n        obj_pts, img_pts, overlay = det\n        pose, err = estimate_pose_pnp(obj_pts, img_pts, self.K, self.dist)\n        poses.append(pose)\n        if self.save_images and overlay is not None:\n            cv2.imwrite(str(image_dir / f\"{idx:03d}_overlay.png\"), overlay)\n        log.debug(\"Frame %d reprojection error %.3f\", idx, err)\n\n    if not poses:\n        log.error(\"No valid poses computed\")\n        return\n\n    out_json = image_dir / \"camera_poses.json\"\n    save_poses(poses, out_json)\n    log.info(\"Calibration finished -&gt; %s\", out_json)\n</code></pre>"},{"location":"api/#calibration.CharucoCalibrator","title":"<code>CharucoCalibrator</code>","text":"<p>Collects frames and performs Charuco intrinsic calibration.</p> Source code in <code>calibration/charuco_intrinsics.py</code> <pre><code>class CharucoCalibrator:\n    \"\"\"Collects frames and performs Charuco intrinsic calibration.\"\"\"\n\n    def __init__(self, board: cv2.aruco.CharucoBoard, dictionary: cv2.aruco_Dictionary) -&gt; None:\n        self.board = board\n        self.dictionary = dictionary\n        self.all_corners: list[np.ndarray] = []\n        self.all_ids: list[np.ndarray] = []\n        self.image_size: Optional[tuple[int, int]] = None\n\n    def add_frame(self, img: np.ndarray) -&gt; bool:\n        \"\"\"Detect Charuco corners and add them for calibration.\"\"\"\n\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        self.image_size = (gray.shape[1], gray.shape[0])\n        corners, ids, _ = cv2.aruco.detectMarkers(gray, self.dictionary)\n        if ids is None or len(corners) == 0:\n            logger.warning(\"No ArUco markers detected\")\n            return False\n        retval, charuco_corners, charuco_ids = cv2.aruco.interpolateCornersCharuco(\n            corners, ids, gray, self.board\n        )\n        if retval &lt; 10 or charuco_ids is None or len(charuco_ids) &lt; 10:\n            logger.warning(\"Not enough Charuco corners\")\n            return False\n        self.all_corners.append(charuco_corners)\n        self.all_ids.append(charuco_ids)\n        return True\n\n    def calibrate(self) -&gt; CalibrationResult:\n        \"\"\"Run OpenCV Charuco calibration.\"\"\"\n\n        if len(self.all_corners) &lt; 3 or self.image_size is None:\n            raise RuntimeError(\"Not enough frames for calibration\")\n        rms, camera_matrix, dist_coeffs, *_ = cv2.aruco.calibrateCameraCharuco(\n            charucoCorners=self.all_corners,\n            charucoIds=self.all_ids,\n            board=self.board,\n            imageSize=self.image_size,\n            cameraMatrix=None,\n            distCoeffs=None,\n        )\n        return CalibrationResult(camera_matrix, dist_coeffs, rms)\n</code></pre>"},{"location":"api/#calibration.CharucoCalibrator.add_frame","title":"<code>add_frame(img)</code>","text":"<p>Detect Charuco corners and add them for calibration.</p> Source code in <code>calibration/charuco_intrinsics.py</code> <pre><code>def add_frame(self, img: np.ndarray) -&gt; bool:\n    \"\"\"Detect Charuco corners and add them for calibration.\"\"\"\n\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    self.image_size = (gray.shape[1], gray.shape[0])\n    corners, ids, _ = cv2.aruco.detectMarkers(gray, self.dictionary)\n    if ids is None or len(corners) == 0:\n        logger.warning(\"No ArUco markers detected\")\n        return False\n    retval, charuco_corners, charuco_ids = cv2.aruco.interpolateCornersCharuco(\n        corners, ids, gray, self.board\n    )\n    if retval &lt; 10 or charuco_ids is None or len(charuco_ids) &lt; 10:\n        logger.warning(\"Not enough Charuco corners\")\n        return False\n    self.all_corners.append(charuco_corners)\n    self.all_ids.append(charuco_ids)\n    return True\n</code></pre>"},{"location":"api/#calibration.CharucoCalibrator.calibrate","title":"<code>calibrate()</code>","text":"<p>Run OpenCV Charuco calibration.</p> Source code in <code>calibration/charuco_intrinsics.py</code> <pre><code>def calibrate(self) -&gt; CalibrationResult:\n    \"\"\"Run OpenCV Charuco calibration.\"\"\"\n\n    if len(self.all_corners) &lt; 3 or self.image_size is None:\n        raise RuntimeError(\"Not enough frames for calibration\")\n    rms, camera_matrix, dist_coeffs, *_ = cv2.aruco.calibrateCameraCharuco(\n        charucoCorners=self.all_corners,\n        charucoIds=self.all_ids,\n        board=self.board,\n        imageSize=self.image_size,\n        cameraMatrix=None,\n        distCoeffs=None,\n    )\n    return CalibrationResult(camera_matrix, dist_coeffs, rms)\n</code></pre>"},{"location":"api/#calibration.save_camera_params","title":"<code>save_camera_params(filename, image_size, camera_matrix, dist_coeffs, total_avg_err)</code>","text":"<p>Save camera parameters and reprojection error to YAML.</p> Source code in <code>calibration/charuco_intrinsics.py</code> <pre><code>def save_camera_params(\n    filename: Path,\n    image_size: tuple[int, int],\n    camera_matrix: np.ndarray,\n    dist_coeffs: np.ndarray,\n    total_avg_err: float,\n) -&gt; None:\n    \"\"\"Save camera parameters and reprojection error to YAML.\"\"\"\n\n    calibration_data = {\n        \"image_width\": image_size[0],\n        \"image_height\": image_size[1],\n        \"camera_matrix\": {\n            \"rows\": camera_matrix.shape[0],\n            \"cols\": camera_matrix.shape[1],\n            \"dt\": \"d\",\n            \"data\": camera_matrix.tolist(),\n        },\n        \"distortion_coefficients\": {\n            \"rows\": dist_coeffs.shape[0],\n            \"cols\": dist_coeffs.shape[1] if dist_coeffs.ndim &gt; 1 else 1,\n            \"dt\": \"d\",\n            \"data\": dist_coeffs.flatten().tolist(),\n        },\n        \"avg_reprojection_error\": float(total_avg_err),\n    }\n    filename.parent.mkdir(parents=True, exist_ok=True)\n    import yaml\n\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(calibration_data, f)\n    logger.info(\"Saved calibration parameters to %s\", filename)\n</code></pre>"},{"location":"api/#utils.D415_Cfg","title":"<code>D415_Cfg</code>  <code>dataclass</code>","text":"<p>Intel RealSense D415 camera configuration: - frame size (color/depth) - frame rate - depth scale - alignment mode</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass D415_Cfg:\n    \"\"\"\n    Intel RealSense D415 camera configuration:\n    - frame size (color/depth)\n    - frame rate\n    - depth scale\n    - alignment mode\n    \"\"\"\n\n    rgb_width: int = 1920\n    rgb_height: int = 1080\n    # rgb_width: int = 1280\n    # rgb_height: int = 720\n    depth_width: int = 1280\n    depth_height: int = 720\n    fps: int = 30\n    # TODO clarify depth_scale import at all python pakages\n    # and del DEPTH_SCALE anywhere\n    depth_scale: float = DEPTH_SCALE\n    align_to_color: bool = True\n</code></pre>"},{"location":"api/#utils.GridCalibCfg","title":"<code>GridCalibCfg</code>  <code>dataclass</code>","text":"<p>Grid-based workspace sampling for hand-eye calibration. Defines the limits, grid step, orientation, and output.</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass GridCalibCfg:\n    \"\"\"\n    Grid-based workspace sampling for hand-eye calibration.\n    Defines the limits, grid step, orientation, and output.\n    \"\"\"\n\n    calibration_type: str = \"EYE_IN_HAND\"\n    workspace_limits: tuple[\n        tuple[float, float], tuple[float, float], tuple[float, float]\n    ] = (\n        (-70.0, 50.0),  # X, mm\n        (-250.0, -130.0),  # Y, mm\n        (300.0, 400.0),  # Z, mm\n    )\n    grid_step: float = 40.0\n    tool_orientation: tuple[float, float, float] = (180.0, 0.0, 180.0)\n    charuco_xml: str = str(paths.CAMERA_INTR / \"charuco_cam.xml\")\n    calib_output_dir: str = str(paths.RESULTS_DIR)\n</code></pre>"},{"location":"api/#utils.HandEyeCfg","title":"<code>HandEyeCfg</code>  <code>dataclass</code>","text":"<p>Hand-eye calibration configuration parameters. Includes Charuco board parameters, allowed outliers, min corners, input/output paths, etc.</p> Source code in <code>utils/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass HandEyeCfg:\n    \"\"\"\n    Hand-eye calibration configuration parameters.\n    Includes Charuco board parameters, allowed outliers,\n    min corners, input/output paths, etc.\n    \"\"\"\n\n    square_numbers: tuple[int, int] = (9, 6)\n    square_length: float = 0.031\n    marker_length: float = 0.023\n    CHARUCO_DICT_MAP = {\n        \"5X5_50\": cv2.aruco.DICT_5X5_50,\n        \"5X5_100\": cv2.aruco.DICT_5X5_100,\n    }\n    aruco_dict: str = \"5X5_100\"\n    min_corners: int = 4\n    outlier_std: float = 2.0\n    method: str = \"ALL\"\n    analyze_corners: bool = False\n    visualize: bool = False\n    robot_poses_file = paths.CAPTURES_EXTR_DIR.glob(\"*.json\")  # 'calib/*.json'\n    images_dir: str = str(paths.CAPTURES_DIR)\n    charuco_xml: str = str(paths.CAMERA_INTR / \"charuco_cam.xml\")\n    charuco_txt: str = str(paths.CAMERA_INTR / \"charuco_cam.txt\")\n    calib_output_dir: str = str(paths.RESULTS_DIR)\n</code></pre>"},{"location":"api/#utils.JSONPoseLoader","title":"<code>JSONPoseLoader</code>","text":"<p>Load robot poses for hand-eye calibration from a JSON file.</p> Source code in <code>utils/io.py</code> <pre><code>class JSONPoseLoader:\n    \"\"\"Load robot poses for hand-eye calibration from a JSON file.\"\"\"\n\n    @staticmethod\n    def load_poses(json_file: str) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Return rotation and translation lists from ``json_file``.\"\"\"\n        data = load_json(json_file)\n        Rs, ts = [], []\n        for pose in data.values():\n            tcp_pose = pose[\"tcp_coords\"]\n            t = np.array(tcp_pose[:3], dtype=np.float64) / 1000.0\n            rx, ry, rz = tcp_pose[3:]\n            R_mat = Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=True).as_matrix()\n            Rs.append(R_mat)\n            ts.append(t)\n        return Rs, ts\n</code></pre>"},{"location":"api/#utils.JSONPoseLoader.load_poses","title":"<code>load_poses(json_file)</code>  <code>staticmethod</code>","text":"<p>Return rotation and translation lists from <code>json_file</code>.</p> Source code in <code>utils/io.py</code> <pre><code>@staticmethod\ndef load_poses(json_file: str) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Return rotation and translation lists from ``json_file``.\"\"\"\n    data = load_json(json_file)\n    Rs, ts = [], []\n    for pose in data.values():\n        tcp_pose = pose[\"tcp_coords\"]\n        t = np.array(tcp_pose[:3], dtype=np.float64) / 1000.0\n        rx, ry, rz = tcp_pose[3:]\n        R_mat = Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=True).as_matrix()\n        Rs.append(R_mat)\n        ts.append(t)\n    return Rs, ts\n</code></pre>"},{"location":"api/#utils.Logger","title":"<code>Logger</code>","text":"<p>Project-wide logger wrapper using loguru and global config.</p> Source code in <code>utils/logger.py</code> <pre><code>class Logger:\n    \"\"\"Project-wide logger wrapper using loguru and global config.\"\"\"\n\n    @staticmethod\n    def _configure(level: str, json_format: bool) -&gt; None:\n        \"\"\"Configure log sinks on first use.\"\"\"\n        global _is_configured, _log_file\n        _logger.remove()\n        os.makedirs(_log_dir, exist_ok=True)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        _log_file = _log_dir / f\"{timestamp}.log.json\"\n        _logger.add(\n            sys.stdout,\n            level=level,\n            serialize=False,\n            format=LOGCFG.log_format,\n        )\n        _logger.add(\n            _log_file,\n            level=level,\n            serialize=json_format,\n            format=LOGCFG.log_file_format,\n        )\n        _is_configured = True\n\n    @staticmethod\n    def get_logger(\n        name: str, level: str = None, json_format: bool = None\n    ) -&gt; LoguruLogger:\n        \"\"\"\n        Return a configured loguru logger bound to ``name``.\n        If level or json_format are not specified, uses global config.\n        \"\"\"\n        global _is_configured\n        if not _is_configured:\n            Logger._configure(\n                level or LOGCFG.level,\n                json_format if json_format is not None else LOGCFG.json,\n            )\n        return _logger.bind(module=name)\n\n    @staticmethod\n    def progress(\n        iterable: Iterable[T],\n        desc: str | None = None,\n        total: int | None = None,\n    ) -&gt; Iterable[T]:\n        \"\"\"Return a tqdm iterator with unified style.\"\"\"\n        return cast(\n            Iterable[T],\n            tqdm(\n                iterable,\n                desc=desc,\n                total=total,\n                leave=False,\n                bar_format=LOGCFG.progress_bar_format,\n            ),\n        )\n\n    @staticmethod\n    def configure_root_logger(level: str = \"WARNING\") -&gt; None:\n        \"\"\"Configure the root logger for third-party libraries.\"\"\"\n        global _is_configured\n        _logger.remove()\n        _logger.add(sys.stdout, level=level)\n        _is_configured = True\n\n    @staticmethod\n    def configure(\n        level: str = None, log_dir: str | Path = None, json_format: bool = None\n    ) -&gt; None:\n        \"\"\"Manually configure the logger with given settings.\"\"\"\n        global _log_dir\n        _log_dir = Path(log_dir) if log_dir is not None else LOGCFG.log_dir\n        Logger._configure(\n            level or LOGCFG.level,\n            json_format if json_format is not None else LOGCFG.json,\n        )\n</code></pre>"},{"location":"api/#utils.Logger.configure","title":"<code>configure(level=None, log_dir=None, json_format=None)</code>  <code>staticmethod</code>","text":"<p>Manually configure the logger with given settings.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef configure(\n    level: str = None, log_dir: str | Path = None, json_format: bool = None\n) -&gt; None:\n    \"\"\"Manually configure the logger with given settings.\"\"\"\n    global _log_dir\n    _log_dir = Path(log_dir) if log_dir is not None else LOGCFG.log_dir\n    Logger._configure(\n        level or LOGCFG.level,\n        json_format if json_format is not None else LOGCFG.json,\n    )\n</code></pre>"},{"location":"api/#utils.Logger.configure_root_logger","title":"<code>configure_root_logger(level='WARNING')</code>  <code>staticmethod</code>","text":"<p>Configure the root logger for third-party libraries.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef configure_root_logger(level: str = \"WARNING\") -&gt; None:\n    \"\"\"Configure the root logger for third-party libraries.\"\"\"\n    global _is_configured\n    _logger.remove()\n    _logger.add(sys.stdout, level=level)\n    _is_configured = True\n</code></pre>"},{"location":"api/#utils.Logger.get_logger","title":"<code>get_logger(name, level=None, json_format=None)</code>  <code>staticmethod</code>","text":"<p>Return a configured loguru logger bound to <code>name</code>. If level or json_format are not specified, uses global config.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef get_logger(\n    name: str, level: str = None, json_format: bool = None\n) -&gt; LoguruLogger:\n    \"\"\"\n    Return a configured loguru logger bound to ``name``.\n    If level or json_format are not specified, uses global config.\n    \"\"\"\n    global _is_configured\n    if not _is_configured:\n        Logger._configure(\n            level or LOGCFG.level,\n            json_format if json_format is not None else LOGCFG.json,\n        )\n    return _logger.bind(module=name)\n</code></pre>"},{"location":"api/#utils.Logger.progress","title":"<code>progress(iterable, desc=None, total=None)</code>  <code>staticmethod</code>","text":"<p>Return a tqdm iterator with unified style.</p> Source code in <code>utils/logger.py</code> <pre><code>@staticmethod\ndef progress(\n    iterable: Iterable[T],\n    desc: str | None = None,\n    total: int | None = None,\n) -&gt; Iterable[T]:\n    \"\"\"Return a tqdm iterator with unified style.\"\"\"\n    return cast(\n        Iterable[T],\n        tqdm(\n            iterable,\n            desc=desc,\n            total=total,\n            leave=False,\n            bar_format=LOGCFG.progress_bar_format,\n        ),\n    )\n</code></pre>"},{"location":"api/#utils.TransformUtils","title":"<code>TransformUtils</code>  <code>dataclass</code>","text":"<p>Utility class for chaining and applying transformations.</p> Source code in <code>utils/geometry.py</code> <pre><code>@dataclass\nclass TransformUtils:\n    \"\"\"Utility class for chaining and applying transformations.\"\"\"\n\n    logger: Logger | None = None\n\n    def __post_init__(self) -&gt; None:  # noqa: D401\n        self.logger = self.logger or Logger.get_logger(\"utils.transform\")\n\n    @staticmethod\n    def build_transform(R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n        return make_transform(R, t)\n\n    @staticmethod\n    def apply_transform(points: np.ndarray, R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n        T = make_transform(R, t)\n        points_h = np.hstack([points, np.ones((points.shape[0], 1))])\n        return (T @ points_h.T).T[:, :3]\n\n    @staticmethod\n    def chain_transforms(*Ts: np.ndarray) -&gt; np.ndarray:\n        T_out = np.eye(4)\n        for T in Ts:\n            T_out = T_out @ T\n        return T_out\n\n    def transform_points(self, points: np.ndarray, T: np.ndarray) -&gt; np.ndarray:\n        self.logger.debug(f\"Applying transform to {points.shape[0]} points\")\n        points_h = np.hstack([points, np.ones((points.shape[0], 1))])\n        transformed = (T @ points_h.T).T[:, :3]\n        self.logger.debug(\n            f\"Transformed points sample: {transformed[:2].tolist()}\"\n        )\n        return transformed\n\n    def base_to_tcp(\n        self, tcp_pose: np.ndarray | tuple[np.ndarray, np.ndarray]\n    ) -&gt; np.ndarray:\n        if isinstance(tcp_pose, np.ndarray) and tcp_pose.shape == (4, 4):\n            return tcp_pose\n        if isinstance(tcp_pose, (tuple, list)) and len(tcp_pose) == 2:\n            R, t = tcp_pose\n            return make_transform(R, t)\n        raise ValueError(\"tcp_pose must be SE(3) 4x4 or (R, t)\")\n\n    def tool_to_tcp(self, tcp_offset: np.ndarray | None) -&gt; np.ndarray:\n        if tcp_offset is None or np.allclose(tcp_offset, 0):\n            return np.eye(4)\n        x, y, z, rx, ry, rz = tcp_offset\n        rot = euler_to_matrix(rx, ry, rz, degrees=True)\n        return make_transform(rot, np.array([x, y, z]))\n\n    def tcp_to_camera(self, R_handeye: np.ndarray, t_handeye: np.ndarray) -&gt; np.ndarray:\n        return make_transform(R_handeye, t_handeye)\n\n    def get_base_to_camera(\n        self,\n        tcp_pose: np.ndarray | tuple[np.ndarray, np.ndarray],\n        tcp_offset: np.ndarray | None,\n        R_handeye: np.ndarray,\n        t_handeye: np.ndarray,\n    ) -&gt; np.ndarray:\n        T_base_tcp = self.base_to_tcp(tcp_pose)\n        T_tcp_tool = self.tool_to_tcp(tcp_offset)\n        T_tool_cam = self.tcp_to_camera(R_handeye, t_handeye)\n        T_base_cam = self.chain_transforms(T_base_tcp, T_tcp_tool, T_tool_cam)\n        self.logger.info(\"Computed T_base\u2192camera\")\n        return T_base_cam\n\n    def camera_to_world(\n        self, points_cam: np.ndarray, T_base_cam: np.ndarray\n    ) -&gt; np.ndarray:\n        self.logger.info(\"Transforming points: camera \u2192 world\")\n        return self.transform_points(points_cam, T_base_cam)\n\n    def world_to_camera(\n        self, points_world: np.ndarray, T_base_cam: np.ndarray\n    ) -&gt; np.ndarray:\n        self.logger.info(\"Transforming points: world \u2192 camera\")\n        return self.transform_points(points_world, invert_transform(T_base_cam))\n\n    def camera_to_tcp(\n        self, points_cam: np.ndarray, R_handeye: np.ndarray, t_handeye: np.ndarray\n    ) -&gt; np.ndarray:\n        T = self.tcp_to_camera(R_handeye, t_handeye)\n        return self.transform_points(points_cam, invert_transform(T))\n\n    def tcp_to_camera_points(\n        self, points_tcp: np.ndarray, R_handeye: np.ndarray, t_handeye: np.ndarray\n    ) -&gt; np.ndarray:\n        T = self.tcp_to_camera(R_handeye, t_handeye)\n        return self.transform_points(points_tcp, T)\n</code></pre>"},{"location":"api/#utils.decompose_transform","title":"<code>decompose_transform(T)</code>","text":"<p>Return rotation matrix and translation vector from a transform.</p> Source code in <code>utils/geometry.py</code> <pre><code>def decompose_transform(T: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return rotation matrix and translation vector from a transform.\"\"\"\n    return T[:3, :3], T[:3, 3]\n</code></pre>"},{"location":"api/#utils.euler_to_matrix","title":"<code>euler_to_matrix(rx, ry, rz, *, degrees=True)</code>","text":"<p>Return a rotation matrix from Euler angles.</p> Source code in <code>utils/geometry.py</code> <pre><code>def euler_to_matrix(\n    rx: float, ry: float, rz: float, *, degrees: bool = True\n) -&gt; np.ndarray:\n    \"\"\"Return a rotation matrix from Euler angles.\"\"\"\n    return Rotation.from_euler(\"xyz\", [rx, ry, rz], degrees=degrees).as_matrix()\n</code></pre>"},{"location":"api/#utils.invert_transform","title":"<code>invert_transform(T)</code>","text":"<p>Return the inverse of a homogeneous transform.</p> Source code in <code>utils/geometry.py</code> <pre><code>def invert_transform(T: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return the inverse of a homogeneous transform.\"\"\"\n    R, t = decompose_transform(T)\n    R_inv = R.T\n    t_inv = -R_inv @ t\n    return make_transform(R_inv, t_inv)\n</code></pre>"},{"location":"api/#utils.load_camera_params","title":"<code>load_camera_params(path)</code>","text":"<p>Read camera matrix and distortion coefficients from OpenCV XML/YAML.</p> Source code in <code>utils/io.py</code> <pre><code>def load_camera_params(path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Read camera matrix and distortion coefficients from OpenCV XML/YAML.\"\"\"\n    fs = cv2.FileStorage(str(path), cv2.FILE_STORAGE_READ)\n    camera_matrix = fs.getNode(\"camera_matrix\").mat()\n    dist_coeffs = fs.getNode(\"dist_coeffs\").mat()\n    fs.release()\n    return camera_matrix, dist_coeffs\n</code></pre>"},{"location":"api/#utils.load_json","title":"<code>load_json(path)</code>","text":"<p>Load JSON data from <code>path</code>.</p> Source code in <code>utils/io.py</code> <pre><code>def load_json(path: str) -&gt; Any:\n    \"\"\"Load JSON data from ``path``.\"\"\"\n    with open(path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#utils.load_npy","title":"<code>load_npy(path)</code>","text":"<p>Load an <code>.npy</code> array.</p> Source code in <code>utils/io.py</code> <pre><code>def load_npy(path: str) -&gt; np.ndarray:\n    \"\"\"Load an ``.npy`` array.\"\"\"\n    return np.load(path)\n</code></pre>"},{"location":"api/#utils.make_transform","title":"<code>make_transform(R, t)</code>","text":"<p>Build a homogeneous transform from <code>R</code> and <code>t</code>.</p> Source code in <code>utils/geometry.py</code> <pre><code>def make_transform(R: np.ndarray, t: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build a homogeneous transform from ``R`` and ``t``.\"\"\"\n    T = np.eye(4)\n    T[:3, :3] = R\n    T[:3, 3] = t.flatten()\n    return T\n</code></pre>"},{"location":"api/#utils.read_image","title":"<code>read_image(path)</code>","text":"<p>Return an image from <code>path</code> or <code>None</code> if loading fails.</p> Source code in <code>utils/io.py</code> <pre><code>def read_image(path: str) -&gt; np.ndarray | None:\n    \"\"\"Return an image from ``path`` or ``None`` if loading fails.\"\"\"\n    return cv2.imread(path)\n</code></pre>"},{"location":"api/#utils.save_camera_params_txt","title":"<code>save_camera_params_txt(path, K, dist, rms=None)</code>","text":"<p>Write camera parameters to a text file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_camera_params_txt(\n    path: str, K: np.ndarray, dist: np.ndarray, rms: float | None = None\n) -&gt; None:\n    \"\"\"Write camera parameters to a text file.\"\"\"\n    with open(path, \"w\") as f:\n        if rms is not None:\n            f.write(f\"RMS Error: {rms:.6f}\\n\")\n        f.write(\"camera_matrix =\\n\")\n        np.savetxt(f, K, fmt=\"%.10f\")\n        f.write(\"dist_coeffs =\\n\")\n        np.savetxt(f, dist.reshape(1, -1), fmt=\"%.10f\")\n</code></pre>"},{"location":"api/#utils.save_camera_params_xml","title":"<code>save_camera_params_xml(path, K, dist)</code>","text":"<p>Write camera parameters to an XML/YAML file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_camera_params_xml(path: str, K: np.ndarray, dist: np.ndarray) -&gt; None:\n    \"\"\"Write camera parameters to an XML/YAML file.\"\"\"\n    fs = cv2.FileStorage(str(path), cv2.FILE_STORAGE_WRITE)\n    fs.write(\"camera_matrix\", K)\n    fs.write(\"dist_coeffs\", dist)\n    fs.release()\n</code></pre>"},{"location":"api/#utils.save_json","title":"<code>save_json(path, data)</code>","text":"<p>Write data as JSON to <code>path</code>.</p> Source code in <code>utils/io.py</code> <pre><code>def save_json(path: str, data: Any) -&gt; None:\n    \"\"\"Write data as JSON to ``path``.\"\"\"\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/#utils.save_npy","title":"<code>save_npy(path, arr)</code>","text":"<p>Save an array to an <code>.npy</code> file.</p> Source code in <code>utils/io.py</code> <pre><code>def save_npy(path: str, arr: np.ndarray) -&gt; None:\n    \"\"\"Save an array to an ``.npy`` file.\"\"\"\n    np.save(path, arr)\n</code></pre>"},{"location":"api/#utils.write_image","title":"<code>write_image(path, img)</code>","text":"<p>Save an image to disk.</p> Source code in <code>utils/io.py</code> <pre><code>def write_image(path: str, img: np.ndarray) -&gt; None:\n    \"\"\"Save an image to disk.\"\"\"\n    cv2.imwrite(path, img)\n</code></pre>"},{"location":"handeye/","title":"HANDEYE","text":"<p>This document provides a structured, mathematical description of the full chain of transformations required to convert 2D pixel measurements and depth values into 3D robot base coordinates. It also outlines how the code base implements each step of the calibration and scanning pipeline.</p>"},{"location":"handeye/#calibration-scanning-pipeline","title":"Calibration &amp; Scanning Pipeline","text":"<ol> <li>Capture RGB\u2011D frames and robot poses \u2013 <code>calibration.capture.capture_dataset</code> combines <code>robot.controller.RobotController</code> with <code>robot_scan.capture.capture_rgbd</code> to record synchronized color/depth images and TCP poses.</li> <li>Detect the calibration pattern \u2013 <code>calibration.chessboard</code> and <code>calibration.charuco</code> locate board corners using OpenCV routines.</li> <li>Estimate board pose \u2013 for each frame <code>cv2.solvePnP</code> is invoked inside <code>calibration.base</code> to obtain the pattern pose relative to the camera.</li> <li>Solve hand\u2011eye \u2013 <code>calibration.charuco</code> and <code>calibration.chessboard</code> assemble camera\u2194robot pairs and call <code>cv2.calibrateHandEye</code>.</li> <li>Build scan graph \u2013 <code>robot_scan.skeleton.skeletonize_plane</code> and <code>robot_scan.graph.build_graph</code> turn a cropped point cloud into a traversal graph for motion planning.</li> <li>Execute motion \u2013 <code>robot_scan.motion.move_l</code> issues linear moves via the RPC interface in <code>robot.controller.RobotController</code>.</li> </ol> <p>The mathematical background for these steps is detailed below.</p>"},{"location":"handeye/#1-full-coordinate-transformation-chain","title":"1. Full Coordinate Transformation Chain","text":"<p>The following transformation chain converts pixel and depth data from camera coordinates into robot base coordinates: $$ \\begin{bmatrix} u \\ v \\ d \\end{bmatrix} \\xrightarrow{\\mathrm{K_{depth}}} \\mathrm{p_{depth}} \\xrightarrow{\\mathrm{T_{depth2rgb}}} \\mathrm{p_{rgb}} \\xrightarrow{\\mathrm{T_{cam2base}}} \\mathrm{p_{base}} $$</p> <p>Each arrow represents a transformation:</p> <ul> <li>\\(\\mathrm{K_{depth}}\\): Intrinsic matrix of the depth camera.</li> <li>\\(\\mathrm{T_{depth2rgb}}\\): Extrinsic calibration between Depth and RGB sensors.</li> <li>\\(\\mathrm{T_{cam2base}}\\): Robot pose transform from RGB sensors of camera through TCP to base.</li> </ul>"},{"location":"handeye/#11-intrinsic-camera-matrix-opencv-model","title":"1.1 Intrinsic Camera Matrix (OpenCV Model)","text":"\\[ \\mathrm{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}  \\] <ul> <li>\\(f_x, f_y\\): Focal lengths in pixels.</li> <li>\\(c_x, c_y\\): Principal point coordinates.</li> </ul> <p>Reference: OpenCV Camera Calibration</p>"},{"location":"handeye/#12-backprojection-from-rgb-image-to-3d","title":"1.2 Backprojection from RGB image to 3D","text":"\\[ \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} (u - c_x) \\cdot \\dfrac{d}{f_x} \\\\ (v - c_y) \\cdot \\dfrac{d}{f_y} \\\\ d \\end{bmatrix} \\qquad \\begin{aligned} d~&amp;-\\text{depth value (m)} \\\\ (u, v)~&amp;-\\text{RGB image coordinates (pixels)} \\\\ (x, y, z)~&amp;-\\text{camera coordinates} \\end{aligned} \\] <p>This converts depth and pixel coordinates into 3D points in the camera coordinate system.</p>"},{"location":"handeye/#2-camera-to-base-transform","title":"2. Camera-to-Base Transform","text":"\\[  \\mathrm{T_{cam2base}} = \\mathrm{T_{tcp2base}} \\times \\mathrm{T_{cam2tcp}}  \\]"},{"location":"handeye/#21-camera-to-tcp-transformation","title":"2.1 Camera-to-TCP Transformation","text":"\\[  \\mathrm{T_{cam2tcp}} = \\begin{bmatrix} R &amp; t\\\\ 0 &amp; 1  \\end{bmatrix}  \\] <p>Rigid transformation from the camera coordinate system to the robot TCP (tool center point).</p> <p>R: Rotation matrix \\((3\\times3)\\).</p> <p>t: Translation vector \\((3\\times1)\\).</p> <p>Reference: cv2.calibrateHandEye</p> \\[ \\mathrm{TARGET\\_POSE} = [   \\underset{mm}{x, y, z},\\    \\overset{degree}{r_x, r_y, r_z} ] \\] <p>Target Pose of in robot base coords.</p> <p>\\([x, y, z]\\): Translation of the object in millimeters.</p> <p>\\([r_x, r_y, r_z]\\): Euler angles (XYZ order) in degrees.</p>"},{"location":"handeye/#22-tcp-to-base-transformation","title":"2.2 TCP-to-Base Transformation","text":"\\[ \\mathrm{T_{tcp2base}} = \\begin{bmatrix} R(r_x, r_y, r_z) &amp; 0.001 \\cdot \\left[ x, y, z\\right]^T \\\\ 0 &amp; 1  \\end{bmatrix}  \\] <p>\\(R(r_x, r_y, r_z)\\): Rotation matrix from Euler angles (converted to radians).</p> <p>\\(0.001 \\cdot \\left[ x, y, z\\right]^T\\) : Converts mm to meters for robotics.</p>"},{"location":"handeye/#23-euler-angle-rotation-matrix","title":"2.3 Euler Angle Rotation Matrix","text":"\\[ R(r_x, r_y, r_z) = \\begin{bmatrix} c_{r_y} c_{r_z} &amp; c_{r_x} s_{r_y} s_{r_z} - s_{r_x} c_{r_z} &amp; c_{r_x} s_{r_y} c_{r_z} + s_{r_x} s_{r_z} \\\\ c_{r_y} s_{r_z} &amp; s_{r_x} s_{r_y} s_{r_z} + c_{r_x} c_{r_z} &amp; c_{r_z} s_{r_x} s_{r_y} - c_{r_x} s_{r_z} \\\\ -s_{r_y} &amp; c_{r_y} s_{r_x} &amp; c_{r_y} c_{r_x} \\end{bmatrix}, \\quad \\begin{aligned} c_{(\\cdot)} = \\cos{\\left[ \\cdot \\right]} \\\\ s_{(\\cdot)} = \\sin{\\left[ \\cdot \\right]} \\end{aligned} \\] <p>This formulation follows XYZ (roll-pitch-yaw) convention.</p>"},{"location":"handeye/#3-hand-eye-calibration","title":"3. Hand-Eye Calibration","text":"<p>OpenCV implements several closed-form methods for solving the hand\u2013eye calibration problem, which is formulated as the matrix equation:</p> \\[ \\mathrm{A}_i \\mathrm{X} = \\mathrm{X} \\mathrm{B}_i \\] <ul> <li>\\(\\mathrm{A}_i = \\mathrm{T_{robot}}^{(i)}\\): the known robot pose from base to TCP at time \\(i\\).</li> <li>\\(\\mathrm{B}_i = \\mathrm{T_{target}}^{(i)}\\): the known pose of a calibration target (e.g., Chess board, Charuco board) relative to the camera at time \\(i\\).</li> <li>\\(\\mathrm{X} = \\mathrm{T_{cam2tcp}}\\): the unknown camera-to-TCP transformation to solve for</li> </ul> <p>This is a form of the Sylvester-type equation, arising in rigid body calibration problems.</p> <p>OpenCV\u2019s <code>cv2.calibrateHandEye</code> solves this problem using one of several algorithms:</p> <ul> <li>Tsai &amp; Lenz (1989): Decoupled rotation/translation.</li> <li>Park &amp; Martin (1994): Screw theory-based method.</li> <li>Horaud et al. (1995): Quaternion optimization.</li> <li>Daniilidis (1999): Dual quaternion method.</li> <li>Andreff et al. (2001): Algebraic approach minimizing reprojection error.</li> </ul> <p>The calibration minimizes:</p> \\[ \\sum_i \\\\| \\mathrm{A}_i \\mathrm{X} - \\mathrm{X} \\mathrm{B}_i \\\\|^2 \\]"},{"location":"handeye/#references","title":"References","text":"<ul> <li>OpenCV: cv2.calibrateHandEye</li> <li>OpenCV Camera Calibration</li> <li>OpenCV Pose Estimation (solvePnP)</li> <li>Wikipedia: Euler Angles</li> <li>Wikipedia: Transformation Matrix</li> <li>Wikipedia: Sylvester Equation</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#calibration-workflow","title":"Calibration Workflow","text":"<p>Calibration utilities operate on captured RGB/D frames and recorded robot poses. A typical session:</p> <ol> <li>Capture data: <code>python -m calibration.runner_camera --count 20</code></li> <li>Move the robot on a grid: <code>python -m calibration.runner_robot</code></li> <li>Run hand\u2013eye calibration: <code>python handeye_charuco.py</code></li> </ol> <p>Results are stored under <code>calibration/results/</code>.</p>"},{"location":"usage/#robot-cli","title":"Robot CLI","text":"<p>The <code>robot</code> package exposes a small command line interface for common tasks:</p> <pre><code>python -m robot.cli record --ip &lt;robot_ip&gt; --captures_dir ./captures\n</code></pre> <p>Arguments are defined in the <code>robot.cli</code> module:</p> <pre><code>    def _add_record_args(parser: argparse.ArgumentParser) -&gt; None:\n        \"\"\"Arguments for the ``record`` subcommand.\"\"\"\n        parser.add_argument(\"--ip\", default=robot.ip, help=\"Robot IP\")\n        parser.add_argument(\n            \"--captures_dir\",\n            default=str(paths.CAPTURES_DIR),\n            help=\"Directory for saved poses\",\n        )\n        parser.add_argument(\"--drag\", action=\"store_true\", help=\"Enable drag teaching mode\")\n        parser.add_argument(\n            \"--db_path\", default=\"robot_data.lmdb\", help=\"LMDB database path\"\n        )\n        parser.add_argument(\"--use-db\", action=\"store_true\", help=\"Use LMDB storage\")\n</code></pre> <p>Use <code>robot.cli restart</code> to reconnect to the controller if the connection drops.</p>"}]}